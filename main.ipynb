{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIMMDebug Log Data Analysis Notebook\n",
    "This notebook displays all of the analysis of the log data that took place in the PRIMMDebug initial research paper.\n",
    "\n",
    "The log data was collected from five schools between December 2024-Month 2025. It is divided into the following sections:\n",
    "1. **Summary statistics:** ...\n",
    "2. **Establishing variables:**...\n",
    "3. **Visualisation of variables:**...\n",
    "4. **Students' written responses:**...\n",
    "\n",
    "All you need to do is run the notebooks in order and the statistics that appear in the paper will be displayed. If there are any issues, please report them in the [Issues section of the GitHub repository](https://github.com/LaurieGale10/primmdebug-log-data-analysis/issues).\n",
    "\n",
    "Before we run anything else, let's first import all of the necessary libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.stage_log import StageLog\n",
    "from classes.exercise_log import ExerciseLog\n",
    "from classes.student_id import StudentId\n",
    "from classes.exercise_classes.exercise import Exercise\n",
    "from classes.processors.exercise_log_processor import ExerciseLogProcessor\n",
    "from classes.processors.stage_log_processor import StageLogProcessor\n",
    "\n",
    "from loading_services.fetch_logs_from_file import fetch_data_from_json\n",
    "from loading_services.parse_logs import *\n",
    "\n",
    "from testing_service.docker_interface import DockerInterface\n",
    "from testing_service.test_report import TestReport\n",
    "from testing_service.docker_interface import DockerInterface\n",
    "\n",
    "from constants import *\n",
    "from notebook_utils import *\n",
    "\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from statistics import median\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "\n",
    "import sklearn\n",
    "import kmedoids\n",
    "\n",
    "EXERCISES: list[Exercise] = parse_exercises(fetch_data_from_json(\"data/exercises\"))\n",
    "STAGE_LOGS: list[StageLog] = parse_stage_logs(fetch_data_from_json(\"data/cleaned_stage_logs\"))\n",
    "EXERCISE_LOGS: list[ExerciseLog] = parse_exercise_logs(STAGE_LOGS, fetch_data_from_json(\"data/cleaned_exercise_logs\"))\n",
    "STUDENT_IDS: list[StudentId] = parse_student_ids(fetch_data_from_json(\"data/student_ids\"))\n",
    "\n",
    "EXERCISE_LOGS_PER_SESSION: dict[int, list[ExerciseLog]] = {}\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    session_number: int = exercise_log.session\n",
    "    EXERCISE_LOGS_PER_SESSION[session_number] = EXERCISE_LOGS_PER_SESSION.get(session_number, []) + [exercise_log]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "### Log Data Summary\n",
    "This data displays the following summary statistics to give information into the scale of the data we collected. We report below on:\n",
    "- Number of exercises (that contain at least one completed PRIMMDebug stage)\n",
    "  - Successful\n",
    "  - Unsuccessful\n",
    "  - Completed\n",
    "  - Per each PRIMMDebug challenge\n",
    "- Number of PRIMMDebug stages.\n",
    "- Time of data collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of attempted PRIMMDebug challenges: {len(EXERCISE_LOGS)}\")\n",
    "\n",
    "test_harness_results: list[bool] = [ExerciseLogProcessor.did_final_program_pass_test(exercise_log) for exercise_log in EXERCISE_LOGS]\n",
    "number_successful_exercises: int = len([test_harness_result for test_harness_result in test_harness_results if test_harness_result])\n",
    "number_unsuccessful_exercises: int = len([test_harness_result for test_harness_result in test_harness_results if not test_harness_result])\n",
    "\n",
    "print(f\"- Number of successfully completed PRIMMDebug challenges: {display_percentage_string(number_successful_exercises, len(EXERCISE_LOGS))}\")\n",
    "print(f\"- Number of unsuccessful PRIMMDebug challenges: {display_percentage_string(number_unsuccessful_exercises, len(EXERCISE_LOGS))}\")\n",
    "\n",
    "number_completed_exercises: int = len([exercise_log for exercise_log in EXERCISE_LOGS if ExerciseLogProcessor.get_last_stage(exercise_log).stage_name == DebuggingStage.modify])\n",
    "print(f\"- Number of entirely completed PRIMMDebug challenges (where students reached the Make stage): {display_percentage_string(number_completed_exercises, len(EXERCISE_LOGS))}\")\n",
    "\n",
    "final_program_states: list[bool] = [ExerciseLogProcessor.is_final_program_erroneous(exercise) for exercise in EXERCISE_LOGS]\n",
    "number_successful_final_program_states: list[bool] = len([final_program_state for final_program_state in final_program_states if final_program_state])\n",
    "print(f\"- Proportion of PRIMMDebug challenges where last program run successfully executed: {display_percentage_string(number_successful_final_program_states, len(EXERCISE_LOGS))}\\n\")\n",
    "\n",
    "total_time: float = sum([ExerciseLogProcessor.get_time_on_exercise(exercise_log) for exercise_log in EXERCISE_LOGS])\n",
    "print(f\"Total time on PRIMMDebug challenges: {datetime.timedelta(seconds=total_time)}\\n\")\n",
    "\n",
    "print(f\"Number of completed PRIMMDebug stages: {len(STAGE_LOGS)}\")\n",
    "\n",
    "#Number of attempts at each PRIMMDebug challenge\n",
    "challenge_attempts: dict[str, int] = {}\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    challenge_attempts[exercise_log.exercise_name] = challenge_attempts.get(exercise_log.exercise_name, 0) + 1\n",
    "challenge_attempts = dict(sorted(challenge_attempts.items(), key=lambda item: item[1], reverse=True)) #Sort by frequency\n",
    "challenge_attempts_fig = px.bar(x = challenge_attempts.keys(), y = challenge_attempts.values(), labels = {\"x\": \"Challenge Name\", \"y\": \"Frequency\"})\n",
    "challenge_attempts_fig.show()\n",
    "\n",
    "#Number of challenges attempted by each student\n",
    "challenges_per_student: dict[str, int] = {}\n",
    "for exercise in EXERCISE_LOGS:\n",
    "    student_id: str = exercise.student_id\n",
    "    challenges_per_student[student_id] = challenges_per_student.get(student_id, 0) + 1\n",
    "challenges_per_student_fig = px.histogram(challenges_per_student.values(), marginal=\"box\", labels = {\"value\": \"Attempted challenges per student\", \"count\": \"Frequency\"})\n",
    "challenges_per_student_fig.show()\n",
    "\n",
    "#Number of stages per PRIMMDebug challenge attempt\n",
    "stages_per_challenge_attempt: list[int] = [len(exercise.stage_logs) for exercise in EXERCISE_LOGS]\n",
    "stages_per_challenge_fig = px.histogram(stages_per_challenge_attempt, marginal=\"box\", labels={\"value\": \"Number of stages\"})\n",
    "stages_per_challenge_fig.show()\n",
    "\n",
    "#Number of challenge attempts per session\n",
    "attempts_per_session: dict[int, int] = {}\n",
    "for session, logs in EXERCISE_LOGS_PER_SESSION.items():\n",
    "    attempts_per_session[session] = len(logs)\n",
    "attempts_per_session_fig = px.bar(x = attempts_per_session.keys(), y = attempts_per_session.values(), labels = {\"x\": \"Session\", \"y\": \"Frequency\"})\n",
    "attempts_per_session_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Demographics\n",
    "\n",
    "Number of students:\n",
    "- By gender\n",
    "- By year group\n",
    "- By school\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of participating students: {len(STUDENT_IDS)}\")\n",
    "\n",
    "gender_split_fig = px.bar(x = get_gender_split().keys(), y = get_gender_split().values(), labels = {\"x\": \"Gender\", \"y\": \"Frequency\"})\n",
    "gender_split_fig.show()\n",
    "\n",
    "year_group_split_fig = px.bar(x = get_year_group_split().keys(), y = get_year_group_split().values(), labels={\"x\": \"Year Group\", \"y\": \"Frequency\"})\n",
    "year_group_split_fig.show()\n",
    "\n",
    "school_split_fig = px.bar(x = get_school_split().keys(), y = get_school_split().values(), labels={\"x\": \"School\", \"y\": \"Frequency\"})\n",
    "school_split_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Variables\n",
    "Now we move onto introducing the variables that underpin our log data analysis. These are:\n",
    "- Time taken\n",
    "  - Per challenge attempt\n",
    "  - Per stage log\n",
    "  - Per PRIMMDebug stage\n",
    "- Correctness\n",
    "- Engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Taken\n",
    "\n",
    "#### Summary Data\n",
    "This contains data on:\n",
    "- Time taken per PRIMMDebug challenge attempt\n",
    "- Time taken per PRIMMDebug stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time taken per PRIMMDebug challenge attempt\n",
    "time_per_challenge_attempt: list[float] = [ExerciseLogProcessor.get_time_on_exercise(exercise) for exercise in EXERCISE_LOGS if hasattr(exercise,\"end_time\")]\n",
    "time_per_challenge_fig = px.histogram(time_per_challenge_attempt, marginal=\"box\", labels={\"value\": \"Time taken (seconds)\", \"count\": \"Count\"})\n",
    "time_per_challenge_fig.show()\n",
    "\n",
    "#Time taken per stage log\n",
    "time_per_stage: list[float] = [StageLogProcessor.get_time_on_stage(stage) for stage in STAGE_LOGS if StageLogProcessor.get_time_on_stage(stage) is not None]\n",
    "time_per_stage_fig = px.histogram(time_per_stage, marginal=\"box\", labels={\"value\": \"Time taken (seconds)\", \"count\": \"Count\"})\n",
    "time_per_stage_fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage-Specific Data\n",
    "This contains more of the interesting data relating to each stage of the PRIMMDebug process, including:\n",
    "- Time taken per PRIMMDebug stage\n",
    "- How this varies over number of sessions\n",
    "- How time taken on PRIMMDebug challenge attempts varies over number of sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time taken for each stage of PRIMMDebug (TODO: Add confidence intervals)\n",
    "time_by_primmdebug_stage: dict[DebuggingStage, list[float]] = {}\n",
    "for stage in STAGE_LOGS:\n",
    "    if stage.stage_name != DebuggingStage.exit:\n",
    "        if stage.stage_name not in time_per_stage:\n",
    "            time_by_primmdebug_stage[stage.stage_name] = [StageLogProcessor.get_time_on_stage(stage)]\n",
    "        else:\n",
    "            time_by_primmdebug_stage[stage.stage_name].append(StageLogProcessor.get_time_on_stage(stage))\n",
    "display_dict = {\"stage\": [], \"time\": []}\n",
    "for key in DebuggingStage:\n",
    "    if key in time_by_primmdebug_stage:\n",
    "        display_dict[\"stage\"].append(key.value)\n",
    "        display_dict[\"time\"].append(median(time_by_primmdebug_stage[key]))\n",
    "\n",
    "time_by_primmdebug_stage_fig = px.bar(display_dict, x=\"stage\", y=\"time\", labels={\"stage\": \"PRIMMDebug stage\", \"time\": \"Median time on stage (seconds)\"}, title=\"Median time spent on each PRIMMDebug stage\")\n",
    "time_by_primmdebug_stage_fig.show()\n",
    "\n",
    "#Time series for time per PRIMMDebug stage\n",
    "median_time_per_stage_per_session = {\"session\": [], \"stage\": [], \"median_time\": []}\n",
    "# Iterate through each session and calculate median time for each stage\n",
    "for session_id, logs in EXERCISE_LOGS_PER_SESSION.items():\n",
    "    stage_times = {stage: [] for stage in DebuggingStage if stage != DebuggingStage.exit}\n",
    "    for log in logs:\n",
    "        for stage_log in log.stage_logs:\n",
    "            if stage_log.stage_name != DebuggingStage.exit:\n",
    "                time_on_stage = StageLogProcessor.get_time_on_stage(stage_log)\n",
    "                if time_on_stage is not None:\n",
    "                    stage_times[stage_log.stage_name].append(time_on_stage)\n",
    "    for stage, times in stage_times.items():\n",
    "        if times:\n",
    "            median_time_per_stage_per_session[\"session\"].append(session_id)\n",
    "            median_time_per_stage_per_session[\"stage\"].append(stage.value)\n",
    "            median_time_per_stage_per_session[\"median_time\"].append(median(times))\n",
    "\n",
    "median_time_per_stage_per_session_df = DataFrame(median_time_per_stage_per_session).sort_values(by=\"session\")\n",
    "median_time_per_stage_per_session_fig = px.line(median_time_per_stage_per_session_df, x=\"session\", y=\"median_time\", color=\"stage\", labels={\"session\": \"Session\", \"median_time\": \"Median time (seconds)\", \"stage\": \"PRIMMDebug stage\"}, title=\"Median time spent on each PRIMMDebug stage per session\")\n",
    "median_time_per_stage_per_session_fig.show()\n",
    "\n",
    "#Time series for time per PRIMMDebug challenge attempt\n",
    "median_time_per_challenge_per_session: dict[int, float] = {}\n",
    "for session_id, logs in EXERCISE_LOGS_PER_SESSION.items():\n",
    "    median_time_per_challenge = median([ExerciseLogProcessor.get_time_on_exercise(log) for log in logs if hasattr(log, \"end_time\")])\n",
    "    median_time_per_challenge_per_session[session_id] = median_time_per_challenge\n",
    "\n",
    "median_time_per_challenge_per_session = dict(sorted(median_time_per_challenge_per_session.items()))\n",
    "median_time_per_challenge_per_session_fig = px.line(x=median_time_per_challenge_per_session.keys(), y=median_time_per_challenge_per_session.values(), labels={\"x\": \"Session\", \"y\": \"Median time (seconds)\"}, title=\"Median time spent on each PRIMMDebug challenge per session\")\n",
    "median_time_per_challenge_per_session_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness of exercise\n",
    "- Per challenge\n",
    "- Per student\n",
    "\n",
    "Also includes number of find the error stages where correct answer was inputted. This could be broken down into:\n",
    "- First time correct answers\n",
    "- Number of attempts taken to correctly identify erroneous line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate Docker container\n",
    "docker_interface: DockerInterface = DockerInterface.get_instance()\n",
    "docker_interface.create_docker_container()\n",
    "test_reports: list[TestReport] = []\n",
    "\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    test_reports.append(ExerciseLogProcessor.test_final_program(exercise_log, docker_interface))\n",
    "\n",
    "total_tests: list[TestReport] = [test_report for test_report in test_reports if test_report is not None]\n",
    "successful_attempts: int = len([test_report for test_report in total_tests if test_report is not None and test_report.n_successful_tests == test_report.n_total_tests])\n",
    "print(f\"Number of successful test harness runs: {display_percentage_string(successful_attempts, len(total_tests))}\")\n",
    "\n",
    "find_error_stages_with_correct_field: list[StageLog] = [stage_log for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.find_error and stage_log.correct is not None]\n",
    "correct_find_error_stages: int = len([stage_log for stage_log in find_error_stages_with_correct_field if stage_log.correct])\n",
    "print(f\"Number of find the error stages where the correct response was entered (for challenges where students had to pinpoint a line): {display_percentage_string(correct_find_error_stages, len(find_error_stages_with_correct_field))}\")\n",
    "docker_interface.close_docker_container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engagement\n",
    "Some contextual information about general engagement with the tool, including:\n",
    "- Time focused on the window\n",
    "- Runtime behaviour:\n",
    "  - Quality/similarity of tests\n",
    "- Number of exercises where Test stage is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final stage of PRIMMDebug challenge attempts\n",
    "challenge_end_stages: dict[str, int] = dict(Counter([ExerciseLogProcessor.get_last_stage(exercise_log).stage_name.name for exercise_log in EXERCISE_LOGS]))\n",
    "final_stage_fig = px.bar(x = list(challenge_end_stages.keys()), y = list(challenge_end_stages.values()), labels = {\"x\": \"Final stage of PRIMMDebug\", \"y\": \"Frequency\"})\n",
    "final_stage_fig.show()\n",
    "\n",
    "#Time spent focused on PRIMMDebug window per exercise\n",
    "time_spent_focused: list[float] = [ExerciseLogProcessor.get_time_focused(exercise) for exercise in EXERCISE_LOGS]\n",
    "time_spent_focused_fig = px.histogram(time_spent_focused, marginal=\"box\", labels={\"value\": \"Time spent focused on PRIMMDebug window\"})\n",
    "time_spent_focused_fig.show()\n",
    "\n",
    "#Challenge attempts where test case panes were viewed\n",
    "exercises_with_test_case_views: int = len([exercise_log for exercise_log in EXERCISE_LOGS if ExerciseLogProcessor.were_test_cases_viewed(exercise_log)]) / len(EXERCISE_LOGS) * 100\n",
    "print(f\"Percentage of exercises where test cases were viewed at some point: {exercises_with_test_case_views:.2f}\")\n",
    "inspect_the_code_test_case_views: int = len([exercise_log for exercise_log in EXERCISE_LOGS if ExerciseLogProcessor.were_test_cases_viewed(exercise_log, [DebuggingStage.inspect_code])])\n",
    "print(f\"- In the Inspect the Code stage: {display_percentage_string(inspect_the_code_test_case_views, len(EXERCISE_LOGS))}\")\n",
    "test_stage_test_case_views: int = len([exercise_log for exercise_log in EXERCISE_LOGS if ExerciseLogProcessor.were_test_cases_viewed(exercise_log, [DebuggingStage.test])])\n",
    "print(f\"- In the Test stage: {display_percentage_string(test_stage_test_case_views, len(EXERCISE_LOGS))}\")\n",
    "\n",
    "#Exercises where test stage is reached\n",
    "exercises_test_stage_reached: int = len([exercise_log for exercise_log in EXERCISE_LOGS if any(stage.stage_name == DebuggingStage.fix_error for stage in exercise_log.stage_logs)])\n",
    "print(f\"Number of PRIMMDebug challenge attempts where Test stage was reached: {display_percentage_string(exercises_test_stage_reached, len(EXERCISE_LOGS))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis\n",
    "\n",
    "The exercise logs are structured in such a way that could benefit from sequence analysis, which can then be clustered. Given the study's focus on reflective debugging, we decided to cluster based on the time spent on each PRIMMDebug stage. Then, the success, quality of written responses, and survey responses can be calculated to see any difference between the clusters.\n",
    "\n",
    "Procedure for clustering analysis was informed by Murphy et al., (2024), Everitt et al., (year) and Frades and Matthiesen (2010). As a result, a few options for each stage was selected in order that could in turn be compared and evaluated.\n",
    "\n",
    "1. **What are we clustering**: Clustering for two main objects is being explored:\n",
    "  - Feature: Median time on PRIMMDebug stage.\n",
    "  - Item: PRIMMDebug challenge attempts\n",
    "  - Item: Students (with potential addition of number of challenges attempted)\n",
    "  - Both only contain continuous data.\n",
    "2. **How does the data need to be transformed**: Some general preprocessing will need to be performed on the data.\n",
    "  - Missing data: Check how much missing data there is (lots of challenge attempts won't get to the final stages - what to do about these? Labelling them as 0 will naturally influence the clusters). Check how much missing data there is.\n",
    "  - For the partioning methods, data will need to be standardised.\n",
    "  - Item-feature matrix also needs to be made for clustering\n",
    "3. **Similarity measure**: Trying a mix of methods that work solely with continuous data, namely Euclidean, Manhattan (more robust to outliers), Minkowski distance, and maybe Pearson?\n",
    "4. **Clustering algorithm**: Three clustering algorithms are being tried and compared: K-means, K-medoids, and agglomerative hierarchical clustering.\n",
    "5. **Evaluation of clusters**: Widely different solutions may indicate abscence of clusters based on current variables. Here's some metrics used, specifically based on (refs) and (refs):\n",
    "   - For cluster quality: Average Silhouette Width and silhouette plot can be used to determine whether each cluster is \"good\", as well as bootstrapping.\n",
    "   - For influence (of a single point): Point biserial index.\n",
    "Values for both measures range from -1-1.\n",
    "6. **Interpretation**: Inspect clusters and give them names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a set of matrices (`DataFrame`s) to act as parameters for clustering. Each of these matrices contains features related to the time spent on each debugging stage, but they're worked out slightly differently:\n",
    "- `non_zero_times_per_stage`: Contains the *total* time spent on each debugging stage, *excluding* challenge attempts didn't complete every debugging stage (n=).\n",
    "- `median_times_per_stage`: Contains the *median* time spent on each debugging stage, *including* challenge attempts that didn't complete every debugging stage (n=).\n",
    "- `non_zero_median_times_per_stage`: Contains the *median* time spent on each debugging stage, *excluding* challenge attempts didn't complete every debugging stage (n=)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_stages: list[DebuggingStage] = [DebuggingStage.completed_test, DebuggingStage.modify, DebuggingStage.make, DebuggingStage.exit]\n",
    "\n",
    "#Contains the times per stage for each exercise log. Not created a DataFrame for this as clustering is not being performed on it\n",
    "times_per_stage: list[dict[DebuggingStage, float]] = []\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    time_per_stage: dict[DebuggingStage, float] = {stage: time for stage, time in ExerciseLogProcessor.get_time_per_stage(exercise_log).items() if stage not in null_stages}\n",
    "    times_per_stage.append(time_per_stage)\n",
    "\n",
    "#Filter zero values\n",
    "non_zero_times_per_stage: list[dict[DebuggingStage, float]] = [time_per_stage for time_per_stage in times_per_stage if all(value > 0 for value in time_per_stage.values())]\n",
    "non_zero_times_per_stage_df: DataFrame = DataFrame.from_dict(non_zero_times_per_stage)\n",
    "\n",
    "median_times_per_stage: list[dict[DebuggingStage, float]] = []\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    time_per_stage: dict[DebuggingStage, float] = {stage: time for stage, time in ExerciseLogProcessor.get_time_per_stage(exercise_log, True).items() if stage not in null_stages}\n",
    "    median_times_per_stage.append(time_per_stage)\n",
    "median_times_per_stage_df: DataFrame = DataFrame.from_dict(median_times_per_stage)\n",
    "\n",
    "non_zero_median_times_per_stage: list[dict[DebuggingStage, float]] = [time_per_stage for time_per_stage in median_times_per_stage if all(value > 0 for value in time_per_stage.values())]\n",
    "non_zero_median_times_per_stage_df: DataFrame = DataFrame.from_dict(non_zero_median_times_per_stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then standardise these values using `StandardScaler.fit_transform` and plot scatter plot matrices to identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "columns = non_zero_times_per_stage_df.columns\n",
    "\n",
    "non_zero_times_per_stage_df = DataFrame(scaler.fit_transform(non_zero_times_per_stage_df), columns=columns)\n",
    "median_times_per_stage_df = DataFrame(scaler.fit_transform(median_times_per_stage_df), columns=columns)\n",
    "non_zero_median_times_per_stage_df = DataFrame(scaler.fit_transform(non_zero_median_times_per_stage_df), columns=columns)\n",
    "\n",
    "px.scatter_matrix(non_zero_times_per_stage_df, dimensions=non_zero_times_per_stage_df.columns, title=\"Scatter matrix of non-zero times per stage (non_zero_times_per_stage_df)\").show()\n",
    "px.scatter_matrix(median_times_per_stage_df, dimensions=median_times_per_stage_df.columns, title=\"Scatter matrix of median times per stage (median_times_per_stage_df)\").show()\n",
    "px.scatter_matrix(non_zero_median_times_per_stage_df, dimensions=non_zero_median_times_per_stage_df.columns, title=\"Scatter matrix of non-zero median times per stage (non_zero_median_times_per_stage_df)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing outliers**\n",
    "\n",
    "Each scatter plot matrix shows a few outliers, which are subsequently removed with the Z score threshold > +- 3.\n",
    "\n",
    "(Note to self: this might prevent identifying which items are in which clusters. If this is the case, programmatically detect and remove outliers before standardising dfs. Hopefully there shouldn't be any Z scores > +- 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold for outliers:\n",
    "non_zero_times_per_stage_df = non_zero_times_per_stage_df[(non_zero_times_per_stage_df.abs() <= 3).all(axis=1)]\n",
    "median_times_per_stage_df = median_times_per_stage_df[(median_times_per_stage_df.abs() <= 3).all(axis=1)]\n",
    "non_zero_median_times_per_stage_df = non_zero_median_times_per_stage_df[(non_zero_median_times_per_stage_df.abs() <= 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "Disadvantages:\n",
    "- `k` has to be determined in advanced (although visual heuristics can be used to help optimise this Murphy et al., (2024)).\n",
    "- Constrained to Euclidean distance. This is not a significant drawback as this was defined as a suitable distance metric given the continuous nature of the data.\n",
    "- Sensitive to initial clusters and can converge to local minima. Can be done by running the algorithm within a large number of random centroids.\n",
    "\n",
    "First, we work out the best value of k from elbow plot from visualising elbow plot of TWCSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K: int = 10\n",
    "k_means_solutions: DataFrame = DataFrame(columns=[\"k\", \"Total within-cluster sum of squares\", \"DataFrame\"])\n",
    "\n",
    "dataframe_names: dict[str, DataFrame] = {\n",
    "    \"Non-zero times per stage\": non_zero_times_per_stage_df,\n",
    "    \"Median times per stage\": median_times_per_stage_df,\n",
    "    \"Non-zero median times per stage\": non_zero_median_times_per_stage_df,\n",
    "}\n",
    "\n",
    "i: int = 0\n",
    "for name, df in dataframe_names.items():\n",
    "    for k in range(1, K + 1):\n",
    "        k_means = KMeans(n_clusters=k, random_state=0).fit(df)\n",
    "        k_means_solutions.loc[i] = [k, k_means.inertia_, name]\n",
    "        if k >= 2:\n",
    "            print(\"Silhouette score for k =\", k, \"on\", name, \":\", sklearn.metrics.silhouette_score(df, k_means.predict(df)))\n",
    "        i += 1\n",
    "px.line(\n",
    "    k_means_solutions,\n",
    "    x = \"k\",\n",
    "    y = \"Total within-cluster sum of squares\",\n",
    "    color=\"DataFrame\",\n",
    "    title=\"Elbow plot for k-means clustering\"#TODO: Could include ASW in title as sanity check\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning\n",
    "From this we can see that the `non_zero_median_times_per_stage` DataFrame consistently has the lowest TWCSS for each `k`, so we use this DataFrame for the rest of the analysis (**is this ok?**). The elbow plot indicates `k=4` is optimal, so we continue by trying different `init` and `n_init` parameter values to see if the TWCSS can be improved any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for init in [\"k-means++\", \"random\"]:\n",
    "    for n_init in [1, 50, 100]:\n",
    "        k_means = KMeans(n_clusters=4, init=init, n_init=n_init, random_state=0).fit(non_zero_median_times_per_stage_df)\n",
    "        print(f\"{init}, {n_init}: {k_means.inertia_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Final Model\n",
    "Shows n_init=50 with k_means++ (default argument) is suitable. We now fit this model and check the number of clusters per item. Evaluation of this model comes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_final_model = KMeans(n_clusters=4, n_init=50, random_state=0)\n",
    "k_means_final_model.fit_transform(non_zero_median_times_per_stage_df)\n",
    "#print(k_means_final_model.cluster_centers_) #TODO: Needs transformation back to original scale\n",
    "items_per_cluster: dict[int, int] = {\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 0\n",
    "}\n",
    "for label in k_means_final_model.labels_:\n",
    "    items_per_cluster[label+1] += 1\n",
    "print(items_per_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model\n",
    "Now we plot an Average Silhouette Plot for the relevant k_means models\n",
    "\n",
    "**Should I evaluate all the different models? Seems like there's too many variables to do this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cluster_labels = k_means_final_model.fit_predict(non_zero_median_times_per_stage_df)\n",
    "silhouette_values = sklearn.metrics.silhouette_samples(non_zero_median_times_per_stage_df, cluster_labels)\n",
    "silhouette_scores = sklearn.metrics.silhouette_score(non_zero_median_times_per_stage_df, k_means.predict(non_zero_median_times_per_stage_df))\n",
    "fig, ax = plot_silhouette_plot(cluster_labels, silhouette_values, silhouette_scores, k_means_final_model.n_clusters)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Medoids\n",
    "\n",
    "Improvements on K-means:\n",
    "- Not constrained to Euclidean distance; other dissimilarity measures can be used.\n",
    "- Less affected by outliers.\n",
    "- Data doesn't have to be continuous.\n",
    "\n",
    "We've already standardised and removed missing values, so the first thing to do is to visualise an elbow plot.\n",
    "\n",
    "Instead of deciding on the DataFrames, we visualise different dissimilarity measures (euclidean, manhattan, and minkowski). We have to pre-create the dissimilarity matrices to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "K: int = 10\n",
    "k_medoids_solutions: DataFrame = DataFrame(columns=[\"k\", \"Total within-cluster sum of squares\", \"Dissimilarity measure\"])\n",
    "\n",
    "i: int = 0\n",
    "for dissimilarity_measure in [\"euclidean\", \"manhattan\", \"minkowski\"]:\n",
    "    df = sklearn.metrics.pairwise_distances(non_zero_median_times_per_stage_df, metric=dissimilarity_measure)\n",
    "    for k in range(1, K + 1):\n",
    "        k_medoids_intermediate = kmedoids.fastpam1(df, k)\n",
    "        k_medoids_solutions.loc[i] = [k, k_medoids_intermediate.loss, dissimilarity_measure]\n",
    "        i += 1\n",
    "\n",
    "px.line(\n",
    "    k_medoids_solutions,\n",
    "    x = \"k\",\n",
    "    y = \"Total within-cluster sum of squares\",\n",
    "    color=\"Dissimilarity measure\",\n",
    "    title=\"Elbow plot for k-means clustering\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's clearly not much of an elbow for this measure (and very little different between the euclidean and minkowski measures), so I'm going to move on for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Hierarchical Clustering\n",
    "\n",
    "Advantages:\n",
    "- Not restricted to continuous data.\n",
    "- Don't have to specify K (but do have to specify a linkage method)\n",
    "\n",
    "Disadvantages:\n",
    "- Complete linkage doesn't perform well with outliers.\n",
    "- Single linkage can lead to \"chaining\" of clusters.\n",
    "\n",
    "Procedure:\n",
    "- Try a variation of linkage criteria and dissimilarity measures.\n",
    "- Visualise the dendrogram and cut at the most appropriate places (could cut at several points).\n",
    "\n",
    "After a manual inspection the following combinations of parameters:\n",
    "- Any model with single linkage\n",
    "- Average linkage and city block dissimilarity\n",
    "- Average linkage and minkowski dissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "linkage_methods: list[str] = [\"complete\", \"average\", \"ward\"]  # NOTE THAT WARD ONLY WORKS WITH EUCLIDEAN DISTANCE\n",
    "for dissimilarity_measure in [\"euclidean\", \"cityblock\", \"minkowski\"]:\n",
    "    for linkage_method in linkage_methods:\n",
    "        if (linkage_method == \"ward\" and dissimilarity_measure != \"euclidean\") or (linkage_method == \"average\" and dissimilarity_measure != \"euclidean\"):\n",
    "            continue  # Ward only works with Euclidean distance\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.title(f\"Dendrogram for {linkage_method} linkage with {dissimilarity_measure} dissimilarity\")\n",
    "        dendrogram(linkage(non_zero_median_times_per_stage_df, method=linkage_method, metric=dissimilarity_measure))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on sequence analysis\n",
    "Aiming to perform some sequence analysis on the PRIMMDebug challenge attempts, as each student undertook a set of steps that can be mapped as a sequence\n",
    "\n",
    "Other ideas:\n",
    "- Could also try sequence analysis from an exercise logs' first find the error stage.\n",
    "\n",
    "Sequence analysis (following guidance from .. (2024)):\n",
    "- Define properties for sequencing:\n",
    "  - **Alphabet**: The PRIMMDebug stages, as defined in `enums.py`\n",
    "  - **Time scheme**: Whenever the PRIMMDebug stage switches (downside is that this doesn't take time into account; how to set a time window that allows this data to be categorised consistently? e.g. if the time window was every 60 seconds, the student could've gone through several PRIMMDebug stages, which would be missed if we just labelled this the last one)\n",
    "  - **Actor**: A single PRIMMDebug challenge attempt.\n",
    "- Create sessions through ordered actions (done automatically in the `stage_logs` attribute of the `ExerciseLogs` object upon loading data)\n",
    "- Trim overly short or long sessions (outliers in terms of times between states or number of states)\n",
    "- Calculate dissimilarity\n",
    "- Fit data to format that clustering/sequencing package requires - *reshaping*\n",
    "- Add a colour palette (not sure how to do this in Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Responses\n",
    "\n",
    "Another aspect of the log data was the written responses that students had to enter for many of the stages. Some descriptive statistics for these responses are displayed here, with qualitative analysis being performed separately.\n",
    "\n",
    "Some things to display (all by PRIMMDebug stage):\n",
    "- Number of responses containing actual words\n",
    "- Average length of response\n",
    "- First ~50 rows of responses table (logic for saving table should be in here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from save_logs import *\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"words\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "english_words = set(words.words(\"en\"))  # Load English words into a set for fast lookup\n",
    "written_responses: list[str] = [response for exercise_responses in [ExerciseLogProcessor.get_written_responses(exercise_log) for exercise_log in EXERCISE_LOGS] for response in exercise_responses]\n",
    "print(f\"Number of written responses: {len(written_responses)}\")\n",
    "\n",
    "responses_with_valid_words: list[str] = []\n",
    "responses_with_invalid_words: list[str] = []\n",
    "\n",
    "for response in written_responses:\n",
    "    tokens = word_tokenize(response.lower())  # Convert to lowercase for case-insensitive matching\n",
    "    # Check if any token is a valid English word\n",
    "    if any(token in english_words for token in tokens):\n",
    "        responses_with_valid_words.append(response)\n",
    "    else:\n",
    "        responses_with_invalid_words.append(response)\n",
    "\n",
    "print(f\"Number of written responses that contain at least one valid English word: {len(responses_with_valid_words)}/{len(written_responses)} ({(len(responses_with_valid_words) / len(written_responses)) * 100:.2f}%)\")\n",
    "\n",
    "number_inspect_code_stages: int = len([stage_log for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.inspect_code])\n",
    "number_no_response_inspect_code_stages: int = len([stage_log for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.inspect_code and StageLogProcessor.does_inspect_the_code_contain_response(stage_log) is False])\n",
    "print(f\"Number of inspect the code stages which contain no response: {display_percentage_string(number_no_response_inspect_code_stages, number_inspect_code_stages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Information\n",
    "Analysis that doesn't fit into the current framework but might come in handy later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stages taken for a PRIMMDebug challenge\n",
    "- Per exercise\n",
    "- Per student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Median number of stages that each student took on the PRIMMDebug challenges they attempted\n",
    "average_stages_per_student: list[int] = []\n",
    "for student in STUDENT_IDS:\n",
    "    student_EXERCISE_LOGS: list[ExerciseLog] = [exercise for exercise in EXERCISE_LOGS if exercise.student_id == student.id]\n",
    "    if len(student_EXERCISE_LOGS) > 0:\n",
    "        average_stages_per_student.append(median([len(exercise.stage_logs) for exercise in student_EXERCISE_LOGS]))\n",
    "average_stages_per_student_fig = px.histogram(average_stages_per_student, marginal=\"box\", labels={\"value\": \"Median number of stages per student\", \"count\": \"Count\"})\n",
    "average_stages_per_student_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Log Stats\n",
    "For relevant PRIMMDebug stages that contain program logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_runs_inspect_the_code_and_test: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in STAGE_LOGS if stage_log.stage_name in [DebuggingStage.inspect_code, DebuggingStage.test] and StageLogProcessor.get_number_of_runs(stage_log) > 0] #Remove stages where there's 0 runs\n",
    "number_of_runs_inspect_the_code: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.inspect_code and StageLogProcessor.get_number_of_runs(stage_log) > 0]\n",
    "number_of_runs_test: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.test and StageLogProcessor.get_number_of_runs(stage_log) > 0]\n",
    "number_of_runs_fig = px.histogram(number_of_runs_inspect_the_code_and_test, marginal=\"box\", labels={\"x\": \"Time taken (seconds)\"})\n",
    "number_of_runs_fig.show()\n",
    "\n",
    "time_between_runs: list[float] = [time for stage_log in STAGE_LOGS if stage_log.stage_name in [DebuggingStage.inspect_code, DebuggingStage.test] for time in StageLogProcessor.get_time_between_runs(stage_log) if StageLogProcessor.get_time_between_runs(stage_log) != []]\n",
    "time_between_runs_fig = px.histogram(time_between_runs, marginal=\"box\", labels={\"x\": \"Time between runs (seconds)\"})\n",
    "time_between_runs_fig.show()\n",
    "\n",
    "runs_per_minute: list[float] = [round(StageLogProcessor.get_runs_per_minute(stage_log), 2) for stage_log in STAGE_LOGS if stage_log.stage_name in [DebuggingStage.inspect_code, DebuggingStage.test]]\n",
    "print(f\"Runs per minute for inspect the code/test stages: {runs_per_minute}\")\n",
    "\n",
    "number_of_inputs: list[list[int]] = [StageLogProcessor.get_number_of_inputs_from_runs(stage_log) for stage_log in STAGE_LOGS if stage_log.stage_name in [DebuggingStage.inspect_code, DebuggingStage.test]]\n",
    "print(f\"Number of inputs per stage for test stages: {number_of_inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Everitt, B.S., Landau, S., Leese, M. and Stahl, D., 2011. Cluster analysis 5th edition Wiley.\n",
    "Frades I, Matthiesen R. Overview on techniques in cluster analysis. Methods Mol Biol. 2010;593:81-107. doi: 10.1007/978-1-60327-194-3_5. PMID: 19957146.\n",
    "Keefe Murphy, Sonsoles López-Pernas, Mohammed Saqr (2024). Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R. In M. Saqr & S. López-Pernas (Eds.), Learning analytics methods and tutorials: A practical guide using R   (pp. 231-283).Springer, Cham. doi: 10.1007/978-3-031-54464-4_8\n",
    "Saqr, M. et al. (2024). Sequence Analysis in Education: Principles, Technique, and Tutorial with R. In: Saqr, M., López-Pernas, S. (eds) Learning Analytics Methods and Tutorials. Springer, Cham. https://doi.org/10.1007/978-3-031-54464-4_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Save logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
