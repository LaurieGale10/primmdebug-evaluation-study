{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIMMDebug Log Data Analysis Notebook\n",
    "This notebook displays all of the analysis of the log data that took place in the PRIMMDebug initial research paper.\n",
    "\n",
    "The log data was collected from five schools between December 2024-February 2025. It is divided into the following sections:\n",
    "1. **Summary statistics:** ...\n",
    "2. **Establishing variables:**...\n",
    "3. **Visualisation of variables:**...\n",
    "4. **Students' written responses:**...\n",
    "\n",
    "All you need to do is run the notebooks in order and the statistics that appear in the paper will be displayed. If there are any issues, please report them in the [Issues section of the GitHub repository](https://github.com/LaurieGale10/primmdebug-log-data-analysis/issues).\n",
    "\n",
    "Before we run anything else, let's first import all of the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.ExerciseLog import ExerciseLog\n",
    "from classes.StageLog import StageLog\n",
    "from classes.StudentId import StudentId\n",
    "from classes.exercise_classes.Exercise import Exercise\n",
    "from classes.processors.ExerciseLogProcessor import ExerciseLogProcessor\n",
    "from classes.processors.StageLogProcessor import StageLogProcessor\n",
    "from classes.processors.ProgramLogProcessor import ProgramLogProcessor\n",
    "\n",
    "from loading_services.fetch_log_from_firebase import *\n",
    "from loading_services.fetch_logs_from_file import fetch_data_from_json\n",
    "\n",
    "from constants import *\n",
    "from notebook_utils import *\n",
    "\n",
    "from loading_services.parse_logs import *\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "exercises: list[Exercise] = parse_exercises(fetch_data_from_json(\"data/exercises\"))\n",
    "stage_logs: list[StageLog] = parse_stage_logs(fetch_data_from_json(\"data/stage_logs\"))\n",
    "exercise_logs: list[ExerciseLog] = parse_exercise_logs(stage_logs, fetch_data_from_json(\"data/exercise_logs\"))\n",
    "student_ids: list[StudentId] = parse_student_ids(fetch_data_from_json(\"data/student_ids\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "### Exercise/Stage Logs\n",
    "This data displays the following summary statistics to give information into the scale of the data we collected. We report below on:\n",
    "- Number of exercises (that contain at least one completed PRIMMDebug stage)\n",
    "  - Successful\n",
    "  - Unsuccessful\n",
    "  - Completed\n",
    "  - Per each PRIMMDebug challenge\n",
    "- Number of PRIMMDebug stages.\n",
    "- Time of data collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of attempted PRIMMDebug challenges: {len(exercise_logs)}\")\n",
    "\n",
    "number_successful_exercises: int = 0\n",
    "print(f\"- Number of resolved PRIMMDebug challenges: {display_percentage_string(number_successful_exercises, len(exercise_logs))}\")\n",
    "number_unsuccessful_exercises: int = 0\n",
    "print(f\"- Number of unresolved PRIMMDebug challenges: {display_percentage_string(number_unsuccessful_exercises, len(exercise_logs))}\")\n",
    "\n",
    "number_completed_exercises: int = len([exercise_log for exercise_log in exercise_logs if ExerciseLogProcessor.get_last_stage(exercise_log).stage_name == DebuggingStage.modify])\n",
    "print(f\"- Number of entirely completed PRIMMDebug challenges (where students reached the Make stage of PRIMMDebug): {display_percentage_string(number_completed_exercises, len(exercise_logs))}\")\n",
    "\n",
    "final_program_states: list[bool] = [ExerciseLogProcessor.is_final_program_erroneous(exercise) for exercise in exercise_logs]\n",
    "number_successful_final_program_states: list[bool] = len([final_program_state for final_program_state in final_program_states if final_program_state])\n",
    "print(f\"- Proportion of PRIMMDebug challenges where last program run successfully executed: {display_percentage_string(number_successful_final_program_states, len(exercise_logs))}\\n\")\n",
    "\n",
    "total_time: float = sum([ExerciseLogProcessor.get_time_on_exercise(exercise_log) for exercise_log in exercise_logs])\n",
    "print(f\"Total time on PRIMMDebug challenges: {datetime.timedelta(seconds=total_time)}\\n\")\n",
    "\n",
    "print(f\"Number of completed PRIMMDebug stages: {len(stage_logs)}\")\n",
    "\n",
    "#Number of attempts at each PRIMMDebug challenge\n",
    "challenge_attempts: dict[str, int] = {}\n",
    "for exercise_log in exercise_logs:\n",
    "    challenge_attempts[exercise_log.exercise_name] = challenge_attempts.get(exercise_log.exercise_name, 0) + 1\n",
    "challenge_attempts = dict(sorted(challenge_attempts.items(), key=lambda item: item[1], reverse=True)) #Sort by frequency\n",
    "challenge_attempts_fig = px.bar(x = challenge_attempts.keys(), y = challenge_attempts.values(), labels = {\"x\": \"Challenge Name\", \"y\": \"Frequency\"})\n",
    "challenge_attempts_fig.show()\n",
    "\n",
    "#Number of challenges attempted by each student\n",
    "challenges_per_student: dict[str, int] = {}\n",
    "for exercise in exercise_logs:\n",
    "    student_id: str = exercise.student_id\n",
    "    challenges_per_student[student_id] = challenges_per_student.get(student_id, 0) + 1\n",
    "challenges_per_student_fig = px.histogram(challenges_per_student.values(), marginal=\"box\", labels = {\"value\": \"Attempted challenges per student\", \"count\": \"Frequency\"})\n",
    "challenges_per_student_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Demographics\n",
    "\n",
    "Number of students:\n",
    "- By gender\n",
    "- By year group\n",
    "- By school\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of participating students: {len(student_ids)}\")\n",
    "\n",
    "gender_split_fig = px.bar(x = get_gender_split().keys(), y = get_gender_split().values(), labels = {\"x\": \"Gender\", \"y\": \"Frequency\"})\n",
    "gender_split_fig.show()\n",
    "\n",
    "year_group_split_fig = px.bar(x = get_year_group_split().keys(), y = get_year_group_split().values(), labels={\"x\": \"Year Group\", \"y\": \"Frequency\"})\n",
    "year_group_split_fig.show()\n",
    "\n",
    "school_split_fig = px.bar(x = get_school_split().keys(), y = get_school_split().values(), labels={\"x\": \"School\", \"y\": \"Frequency\"})\n",
    "school_split_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Variables\n",
    "Now we move onto introducing the variables that underpin our log data analysis. These include:\n",
    "\n",
    "### Time Taken\n",
    "- Per challenge attempt\n",
    "- Per stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time taken per PRIMMDebug challenge attempt\n",
    "time_per_challenge_attempt: list[float] = [ExerciseLogProcessor.get_time_on_exercise(exercise) for exercise in exercise_logs if hasattr(exercise,\"end_time\")]\n",
    "time_per_challenge_fig = px.histogram(time_per_challenge_attempt, marginal=\"box\", labels={\"value\": \"Time taken (seconds)\", \"count\": \"Count\"})\n",
    "time_per_challenge_fig.show()\n",
    "\n",
    "#Time taken per PRIMMDebug stage\n",
    "time_per_stage: list[float] = [StageLogProcessor.get_time_on_stage(stage) for stage in stage_logs if StageLogProcessor.get_time_on_stage(stage) is not None]\n",
    "time_per_stage_fig = px.histogram(time_per_stage, marginal=\"box\", labels={\"value\": \"Time taken (seconds)\", \"count\": \"Count\"})\n",
    "time_per_stage_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness of exercise\n",
    "- Per challenge\n",
    "- Per student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Correctness of PRIMMDebug challenges:\")\n",
    "print(f\"- Per PRIMMDebug challenge\")\n",
    "print(f\"- Per student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stages taken for a PRIMMDebug challenge\n",
    "- Per exercise\n",
    "- Per student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "\n",
    "#Number of stages per PRIMMDebug challenge attempt\n",
    "stages_per_challenge_attempt: list[int] = [len(exercise.stage_logs) for exercise in exercise_logs]\n",
    "stages_per_challenge_fig = px.histogram(stages_per_challenge_attempt, marginal=\"box\", labels={\"value\": \"Number of stages\"})\n",
    "stages_per_challenge_fig.show()\n",
    "\n",
    "#Median number of stages that each student took on the PRIMMDebug challenges they attempted\n",
    "average_stages_per_student: list[int] = []\n",
    "for student in student_ids:\n",
    "    student_exercise_logs: list[ExerciseLog] = [exercise for exercise in exercise_logs if exercise.student_id == student.id]\n",
    "    if len(student_exercise_logs) > 0:\n",
    "        average_stages_per_student.append(median([len(exercise.stage_logs) for exercise in student_exercise_logs]))\n",
    "average_stages_per_student_fig = px.histogram(average_stages_per_student, marginal=\"box\", labels={\"value\": \"Median number of stages per student\", \"count\": \"Count\"})\n",
    "average_stages_per_student_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Log Stats\n",
    "Placeholder for exercise log stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final stage of PRIMMDebug challenge attempts\n",
    "challenge_end_stages: dict[str, int] = dict(Counter([ExerciseLogProcessor.get_last_stage(exercise_log).stage_name.name for exercise_log in exercise_logs]))\n",
    "final_stage_fig = px.bar(x = list(challenge_end_stages.keys()), y = list(challenge_end_stages.values()), labels = {\"x\": \"Final stage of PRIMMDebug\", \"y\": \"Frequency\"})\n",
    "final_stage_fig.show()\n",
    "\n",
    "#Time spent focused on PRIMMDebug window per exercise\n",
    "time_spent_focused: list[float] = [ExerciseLogProcessor.get_time_focused(exercise) for exercise in exercise_logs]\n",
    "time_spent_focused_fig = px.histogram(time_spent_focused, marginal=\"box\", labels={\"x\": \"100% Time spent focused on PRIMMDebug window\"})\n",
    "time_spent_focused_fig.show()\n",
    "\n",
    "#Challenge attempts where test case panes were viewed\n",
    "exercises_with_test_case_views: int = len([exercise_log for exercise_log in exercise_logs if ExerciseLogProcessor.were_test_cases_viewed(exercise_log)]) / len(exercise_logs) * 100\n",
    "print(f\"Percentage of exercises where test cases were viewed at some point: {exercises_with_test_case_views:.2f}%\")\n",
    "inspect_the_code_test_case_views: int = len([exercise_log for exercise_log in exercise_logs if ExerciseLogProcessor.were_test_cases_viewed(exercise_log, [DebuggingStage.inspect_code])])\n",
    "print(f\"- In the Inspect the Code stage: {display_percentage_string(inspect_the_code_test_case_views, len(exercise_logs))}%\")\n",
    "test_stage_test_case_views: int = len([exercise_log for exercise_log in exercise_logs if ExerciseLogProcessor.were_test_cases_viewed(exercise_log, [DebuggingStage.test])])\n",
    "print(f\"- In the Test stage: {display_percentage_string(test_stage_test_case_views, len(exercise_logs))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Log Stats\n",
    "Placeholder for stage log stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_inspect_code_stages: int = len([stage_log for stage_log in stage_logs if stage_log.stage_name == DebuggingStage.inspect_code])\n",
    "number_no_response_inspect_code_stages: int = len([stage_log for stage_log in stage_logs if stage_log.stage_name == DebuggingStage.inspect_code and StageLogProcessor.does_inspect_the_code_contain_response(stage_log) is False])\n",
    "print(f\"Number of inspect the code stages which contain no response: {display_percentage_string(number_no_response_inspect_code_stages, number_inspect_code_stages)}\")\n",
    "\n",
    "find_error_stages_with_correct_field: list[StageLog] = [stage_log for stage_log in stage_logs if stage_log.stage_name == DebuggingStage.find_error and stage_log.correct is not None]\n",
    "correct_find_error_stages: int = len([stage_log for stage_log in find_error_stages_with_correct_field if stage_log.correct])\n",
    "print(f\"Number of find the error stages where the correct response was entered (for challenges where students had to pinpoint a line): {display_percentage_string(correct_find_error_stages, len(find_error_stages_with_correct_field))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Log Stats\n",
    "For relevant PRIMMDebug stages that contain program logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_runs_inspect_the_code_and_test: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in stage_logs if stage_log.stage_name in [DebuggingStage.inspect_code, DebuggingStage.test] and StageLogProcessor.get_number_of_runs(stage_log) > 0] #Remove stages where there's 0 runs\n",
    "number_of_runs_inspect_the_code: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in stage_logs if stage_log.stage_name == DebuggingStage.inspect_code and StageLogProcessor.get_number_of_runs(stage_log) > 0]\n",
    "number_of_runs_test: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in stage_logs if stage_log.stage_name == DebuggingStage.test and StageLogProcessor.get_number_of_runs(stage_log) > 0]\n",
    "number_of_runs_fig = px.histogram(number_of_runs_inspect_the_code_and_test, marginal=\"box\", labels={\"x\": \"Time taken (seconds)\"})\n",
    "number_of_runs_fig.show()\n",
    "\n",
    "time_between_runs: list[float] = [time for stage_log in stage_logs if stage_log.stage_name in [DebuggingStage.inspect_code, DebuggingStage.test] for time in StageLogProcessor.get_time_between_runs(stage_log) if StageLogProcessor.get_time_between_runs(stage_log) != []]\n",
    "time_between_runs_fig = px.histogram(time_between_runs, marginal=\"box\", labels={\"x\": \"Time between runs (seconds)\"})\n",
    "time_between_runs_fig.show()\n",
    "\n",
    "runs_per_minute: list[float] = [round(StageLogProcessor.get_runs_per_minute(stage_log), 2) for stage_log in stage_logs if stage_log.stage_name in [DebuggingStage.inspect_code, DebuggingStage.test]]\n",
    "print(f\"Runs per minute for inspect the code/test stages: {runs_per_minute}\")\n",
    "\n",
    "number_of_inputs: list[list[int]] = [StageLogProcessor.get_number_of_inputs_from_runs(stage_log) for stage_log in stage_logs if stage_log.stage_name in [DebuggingStage.inspect_code, DebuggingStage.test]]\n",
    "print(f\"Number of inputs per stage for test stages: {number_of_inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Responses\n",
    "\n",
    "For now, just group written responses by stage name and investigate them. Also get some stats on written responses for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from save_logs import *\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"words\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "#save_written_responses(exercise_logs)\n",
    "\n",
    "english_words = set(words.words(\"en\"))  # Load English words into a set for fast lookup\n",
    "written_responses: list[str] = [response for exercise_responses in [ExerciseLogProcessor.get_written_responses(exercise_log) for exercise_log in exercise_logs] for response in exercise_responses]\n",
    "print(f\"Number of written responses: {len(written_responses)}\")\n",
    "\n",
    "responses_with_valid_words: list[str] = []\n",
    "responses_with_invalid_words: list[str] = []\n",
    "\n",
    "for response in written_responses:\n",
    "    tokens = word_tokenize(response.lower())  # Convert to lowercase for case-insensitive matching\n",
    "    # Check if any token is a valid English word\n",
    "    if any(token in english_words for token in tokens):\n",
    "        responses_with_valid_words.append(response)\n",
    "    else:\n",
    "        responses_with_invalid_words.append(response)\n",
    "\n",
    "print(f\"Number of written responses that contain at least one valid English word: {len(responses_with_valid_words)}/{len(written_responses)} ({(len(responses_with_valid_words) / len(written_responses)) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage-specific stuff:\n",
    "- Success rate of students who didn't write anything for inspect the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
