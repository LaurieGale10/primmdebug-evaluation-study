{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIMMDebug Log Data Analysis Notebook\n",
    "This notebook displays all of the analysis of the log data that took place in the PRIMMDebug initial research paper.\n",
    "\n",
    "The log data was collected from five schools between December 2024-Month 2025. It is divided into the following sections:\n",
    "1. **Summary statistics:** An overview of the scale of the data and a breakdown of the participants.\n",
    "2. **Establishing variables:** A closer look at some variables of interest, including time on task and success on the PRIMMDebug challenges.\n",
    "3. **Cluster analysis:** Documentation of the clustering process.\n",
    "\n",
    "All you need to do is run the notebooks in order and the statistics that appear in the paper will be displayed. If there are any issues, please report them in the [Issues section of the GitHub repository](https://github.com/LaurieGale10/primmdebug-log-data-analysis/issues).\n",
    "\n",
    "Before we run anything else, let's first import all of the necessary libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading logs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/exercises.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pointbiserialr\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading logs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m EXERCISES: \u001b[38;5;28mlist\u001b[39m[Exercise] = parse_exercises(\u001b[43mfetch_data_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../../data/exercises\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     37\u001b[39m STAGE_LOGS: \u001b[38;5;28mlist\u001b[39m[StageLog] = parse_stage_logs(fetch_data_from_json(\u001b[33m\"\u001b[39m\u001b[33m../../data/stage_logs\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     38\u001b[39m EXERCISE_LOGS: \u001b[38;5;28mlist\u001b[39m[ExerciseLog] = parse_exercise_logs(STAGE_LOGS, fetch_data_from_json(\u001b[33m\"\u001b[39m\u001b[33m../../data/exercise_logs\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Laurie Gale\\OneDrive - University of Cambridge\\Documents\\PhD Computer Science\\Year 3\\Study 3\\Log Data Analysis\\primmdebug-log-data-analysis\\analysis\\log_data_analysis\\loading_services\\fetch_logs_from_file.py:4\u001b[39m, in \u001b[36mfetch_data_from_json\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_data_from_json\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      5\u001b[39m         data = json.load(file)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../data/exercises.json'"
     ]
    }
   ],
   "source": [
    "from analysis.log_data_analysis.classes.stage_log import StageLog\n",
    "from analysis.log_data_analysis.classes.exercise_log import ExerciseLog\n",
    "from analysis.log_data_analysis.classes.student_id import StudentId\n",
    "from analysis.log_data_analysis.classes.exercise_classes.exercise import Exercise\n",
    "from analysis.log_data_analysis.classes.processors.exercise_log_processor import ExerciseLogProcessor\n",
    "from analysis.log_data_analysis.classes.processors.stage_log_processor import StageLogProcessor\n",
    "\n",
    "from analysis.log_data_analysis.loading_services.fetch_logs_from_file import fetch_data_from_json\n",
    "from analysis.log_data_analysis.loading_services.parse_logs import *\n",
    "\n",
    "from analysis.log_data_analysis.testing_service.docker_interface import DockerInterface\n",
    "from analysis.log_data_analysis.testing_service.test_report import TestReport\n",
    "\n",
    "from analysis.log_data_analysis.constants import *\n",
    "from analysis.log_data_analysis.notebook_utils import *\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "from statistics import median\n",
    "from math import isnan\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "#Libraries for clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from kmedoids import *\n",
    "from pandas import DataFrame, read_csv, cut\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "\n",
    "print(\"Loading logs\")\n",
    "EXERCISES: list[Exercise] = parse_exercises(fetch_data_from_json(\"data/exercises\"))\n",
    "STAGE_LOGS: list[StageLog] = parse_stage_logs(fetch_data_from_json(\"data/stage_logs\"))\n",
    "EXERCISE_LOGS: list[ExerciseLog] = parse_exercise_logs(STAGE_LOGS, fetch_data_from_json(\"data/exercise_logs\"))\n",
    "STUDENT_IDS: list[StudentId] = parse_student_ids(fetch_data_from_json(\"data/student_ids\"), EXERCISE_LOGS)\n",
    "\n",
    "#Initiate Docker container\n",
    "run_tests: bool = False\n",
    "if run_tests:\n",
    "    print(\"Adding test harness data\")\n",
    "    docker_interface: DockerInterface = DockerInterface.get_instance()\n",
    "    docker_interface.create_docker_container()\n",
    "    test_reports: list[TestReport] = []\n",
    "\n",
    "    for exercise_log in EXERCISE_LOGS:\n",
    "        exercise_test_report: TestReport = ExerciseLogProcessor.test_final_program(exercise_log, docker_interface)\n",
    "        if exercise_test_report is not None:\n",
    "            test_reports.append(exercise_test_report)\n",
    "            exercise_log._test_report = exercise_test_report\n",
    "\n",
    "    docker_interface.close_docker_container()\n",
    "\n",
    "print(\"Adding session data\")\n",
    "EXERCISE_LOGS_PER_SESSION: dict[int, list[ExerciseLog]] = {}\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    exercise_log = ExerciseLogProcessor.add_correctness_to_test_stages(exercise_log)\n",
    "    session_number: int = exercise_log.session\n",
    "    EXERCISE_LOGS_PER_SESSION[session_number] = EXERCISE_LOGS_PER_SESSION.get(session_number, []) + [exercise_log]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "### Log Data Summary\n",
    "This data displays the following summary statistics to give information into the scale of the data we collected. We report below on:\n",
    "- Number of exercises\n",
    "  - Per student\n",
    "  - Per session\n",
    "- Number of PRIMMDebug stages\n",
    "  - Per challenge attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of attempted PRIMMDebug challenges: {len(EXERCISE_LOGS)}\")\n",
    "print(f\"Number of completed PRIMMDebug stages: {len(STAGE_LOGS)}\")\n",
    "total_time: float = sum([ExerciseLogProcessor.get_time_on_exercise(exercise_log) for exercise_log in EXERCISE_LOGS])\n",
    "print(f\"Total time on PRIMMDebug challenges: {datetime.timedelta(seconds=total_time)}\\n\")\n",
    "\n",
    "print(f\"Number of students who consented to taking part in the study: {11+16+25+26+16}\")\n",
    "print(f\"Total number of students who attempted at least one PRIMMDebug challenge: {len(STUDENT_IDS)}\\n\") #TODO: Clean number of student IDs to those who only attempted one exercise\n",
    "\n",
    "#Number of attempts at each PRIMMDebug challenge\n",
    "challenge_attempts: dict[str, int] = {}\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    challenge_attempts[exercise_log.exercise_name] = challenge_attempts.get(exercise_log.exercise_name, 0) + 1\n",
    "challenge_attempts = dict(sorted(challenge_attempts.items(), key=lambda item: item[1], reverse=True)) #Sort by frequency\n",
    "px.bar(\n",
    "    x = challenge_attempts.keys(),\n",
    "    y = challenge_attempts.values(),\n",
    "    labels = {\"x\": \"Challenge Name\", \"y\": \"Frequency\"},\n",
    "    title=f\"Number of attempts for each PRIMMDebug challenge (n={len(EXERCISE_LOGS)})\"\n",
    ").show()\n",
    "\n",
    "#Number of challenges attempted by each student\n",
    "challenges_per_student: dict[str, int] = {}\n",
    "for exercise in EXERCISE_LOGS:\n",
    "    student_id: str = exercise.student_id\n",
    "    challenges_per_student[student_id] = challenges_per_student.get(student_id, 0) + 1\n",
    "px.histogram(\n",
    "    challenges_per_student.values(),\n",
    "    marginal=\"box\",\n",
    "    labels={\"value\": \"Number of challenges\", \"count\": \"Frequency\"},\n",
    "    title=f\"Number of attempted PRIMMDebug challenges, broken down by student (n={len(challenges_per_student)})\"\n",
    ").show()\n",
    "\n",
    "# Number of stages per PRIMMDebug challenge attempt\n",
    "stages_per_challenge_attempt: list[int] = [len(exercise.stage_logs) for exercise in EXERCISE_LOGS]\n",
    "px.histogram(\n",
    "    stages_per_challenge_attempt,\n",
    "    marginal=\"box\",\n",
    "    labels={\"value\": \"Number of stages\", \"count\": \"Frequency\"},\n",
    "    title=f\"Number of stages per PRIMMDebug challenge attempt (n={len(stages_per_challenge_attempt)})\"\n",
    ").show()\n",
    "\n",
    "# Number of challenge attempts per session\n",
    "attempts_per_session: dict[int, int] = {}\n",
    "for session, logs in EXERCISE_LOGS_PER_SESSION.items():\n",
    "    attempts_per_session[session] = len(logs)\n",
    "px.bar(\n",
    "    x=attempts_per_session.keys(),\n",
    "    y=attempts_per_session.values(),\n",
    "    labels={\"x\": \"Session\", \"y\": \"Frequency\"},\n",
    "    title=f\"Number of PRIMMDebug challenges in each session (n={sum(attempts_per_session.values())})\"\n",
    ").show()\n",
    "\n",
    "# Final stage of PRIMMDebug challenge attempts\n",
    "challenge_end_stages: dict[str, int] = dict(Counter([ExerciseLogProcessor.get_last_stage(exercise_log).stage_name.name for exercise_log in EXERCISE_LOGS]))\n",
    "challenge_end_stages = {stage.name: challenge_end_stages.get(stage.name, 0) for stage in DebuggingStage if stage.name not in [\"completed_test\", \"exit\"]}\n",
    "px.bar(\n",
    "    x=list(challenge_end_stages.keys()),\n",
    "    y=list(challenge_end_stages.values()),\n",
    "    labels={\"x\": \"PRIMMDebug stage\", \"y\": \"Frequency\"},\n",
    "    title=f\"Final stage of PRIMMDebug challenge attempts (n={len(EXERCISE_LOGS)})\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Demographics\n",
    "\n",
    "Number of students:\n",
    "- By gender\n",
    "- By year group\n",
    "- By school\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of participating students: {len(STUDENT_IDS)}\") #TODO: Implement as csv file to save\n",
    "school_split_fig = px.bar(x = get_school_split().keys(), y = get_school_split().values(), labels={\"x\": \"School\", \"y\": \"Frequency\"})\n",
    "school_split_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Variables\n",
    "Now we move onto introducing the variables that underpin our log data analysis. These are:\n",
    "- Time taken\n",
    "  - Per challenge attempt\n",
    "    - Per session\n",
    "  - Per stage log\n",
    "  - Per PRIMMDebug stage\n",
    "    - Per session\n",
    "- Correctness\n",
    "- Engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Taken\n",
    "\n",
    "First, we present the distribution of times taken for each PRIMMDebug challenge and stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Time taken per PRIMMDebug challenge attempt\n",
    "time_per_challenge_attempt: list[float] = [ExerciseLogProcessor.get_time_on_exercise(exercise) for exercise in EXERCISE_LOGS if hasattr(exercise,\"end_time\")]\n",
    "import scipy.stats as stats\n",
    "\n",
    "skewness = stats.skew(time_per_challenge_attempt, nan_policy='omit')\n",
    "kurtosis = stats.kurtosis(time_per_challenge_attempt, nan_policy='omit')\n",
    "sd = np.nanstd(time_per_challenge_attempt)\n",
    "\n",
    "px.histogram(\n",
    "    time_per_challenge_attempt, marginal=\"box\",\n",
    "    labels={\"value\": \"Time taken (seconds)\", \"count\": \"Count\"},\n",
    "    title=f\"Time taken per PRIMMDebug challenge (skewness={skewness:.2f}, kurtosis={kurtosis:.2f}, std={sd:.2f})\"\n",
    ").show()\n",
    "\n",
    "#Time series for time per PRIMMDebug challenge attempt\n",
    "median_time_per_challenge_per_session: dict[int, float] = {}\n",
    "for session_id, logs in EXERCISE_LOGS_PER_SESSION.items():\n",
    "    median_time_per_challenge = median([ExerciseLogProcessor.get_time_on_exercise(log) for log in logs if hasattr(log, \"end_time\")])\n",
    "    median_time_per_challenge_per_session[session_id] = median_time_per_challenge\n",
    "\n",
    "median_time_per_challenge_per_session = dict(sorted(median_time_per_challenge_per_session.items()))\n",
    "px.line(\n",
    "    x=median_time_per_challenge_per_session.keys(),\n",
    "    y=median_time_per_challenge_per_session.values(),\n",
    "    labels={\"x\": \"Session\", \"y\": \"Median time (seconds)\"},\n",
    "    title=\"Median time spent on each PRIMMDebug challenge per session\"\n",
    ").show()\n",
    "\n",
    "#Time taken per stage log\n",
    "time_per_stage: list[float] = [StageLogProcessor.get_time_on_stage(stage) for stage in STAGE_LOGS if StageLogProcessor.get_time_on_stage(stage) is not None]\n",
    "skewness_stage = stats.skew(time_per_stage, nan_policy='omit')\n",
    "kurtosis_stage = stats.kurtosis(time_per_stage, nan_policy='omit')\n",
    "std_stage = np.nanstd(time_per_stage)\n",
    "\n",
    "px.histogram(\n",
    "    time_per_stage,\n",
    "    marginal=\"box\",\n",
    "    labels={\"value\": \"Time taken (seconds)\", \"count\": \"Count\"},\n",
    "    title=f\"Time taken per PRIMMDebug stage (skewness={skewness_stage:.2f}, kurtosis={kurtosis_stage:.2f}, std={std_stage:.2f})\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time taken per PRIMMDebug stage\n",
    "This contains more of the interesting data relating to each stage of the PRIMMDebug process, including:\n",
    "- Time taken per PRIMMDebug stage\n",
    "  - As a histogram\n",
    "  - As a bar chart\n",
    "- How this varies over number of sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#Time taken for each stage of PRIMMDebug (TODO: Add confidence intervals)\n",
    "time_by_primmdebug_stage = {\"stage\": [], \"time\": []}\n",
    "\n",
    "for stage in STAGE_LOGS:\n",
    "    if stage.stage_name != DebuggingStage.exit:\n",
    "        time_by_primmdebug_stage[\"stage\"].append(stage.stage_name.value)\n",
    "        time_by_primmdebug_stage[\"time\"].append(StageLogProcessor.get_time_on_stage(stage))\n",
    "px.histogram(time_by_primmdebug_stage, x=\"time\", color=\"stage\", nbins=50, marginal=\"box\", labels={\"value\": \"Time taken (seconds)\", \"count\": \"Count\"}, title=\"Time taken per PRIMMDebug stage\").show()\n",
    "\n",
    "# Group times by stage\n",
    "stage_times = defaultdict(list)\n",
    "for stage, time in zip(time_by_primmdebug_stage[\"stage\"], time_by_primmdebug_stage[\"time\"]):\n",
    "    if time is not None:\n",
    "        stage_times[stage].append(time)\n",
    "\n",
    "median_time_by_primmdebug_stage = {\"stage\": [], \"time\": []}\n",
    "for stage in DebuggingStage:\n",
    "    stage_name = stage.value\n",
    "    if stage_name in stage_times and stage_times[stage_name]:\n",
    "        median_time_by_primmdebug_stage[\"stage\"].append(stage_name)\n",
    "        median_time_by_primmdebug_stage[\"time\"].append(median(stage_times[stage_name]))\n",
    "\n",
    "# Print skewness for each stage in median_time_by_primmdebug_stage\n",
    "for stage, median_time in zip(median_time_by_primmdebug_stage[\"stage\"], median_time_by_primmdebug_stage[\"time\"]):\n",
    "    stage_times_list = stage_times[stage]\n",
    "    stage_skewness = stats.skew(stage_times_list, nan_policy='omit')\n",
    "    print(f\"Skewness of times spent on the {stage} stage: {stage_skewness:.2f}\")\n",
    "\n",
    "px.bar(\n",
    "    median_time_by_primmdebug_stage,\n",
    "    x=\"stage\",\n",
    "    y=\"time\",\n",
    "    labels={\"stage\": \"PRIMMDebug stage\", \"time\": \"Median time on stage (seconds)\"},\n",
    "    height=700,\n",
    "    width=550\n",
    ").show()\n",
    "import scipy.stats as stats\n",
    "predict_times = [time for stage, time in zip(time_by_primmdebug_stage[\"stage\"], time_by_primmdebug_stage[\"time\"]) if stage == \"predict\" and time is not None]\n",
    "predict_skewness = stats.skew(predict_times, nan_policy='omit')\n",
    "print(f\"Skewness of times spent on the predict stages: {predict_skewness:.2f}\")\n",
    "\n",
    "#Time series for time per PRIMMDebug stage\n",
    "median_time_per_stage_per_session = {\"session\": [], \"stage\": [], \"median_time\": []}\n",
    "# Iterate through each session and calculate median time for each stage\n",
    "for session_id, logs in EXERCISE_LOGS_PER_SESSION.items():\n",
    "    stage_times = {stage: [] for stage in DebuggingStage if stage != DebuggingStage.exit}\n",
    "    for log in logs:\n",
    "        for stage_log in log.stage_logs:\n",
    "            if stage_log.stage_name != DebuggingStage.exit:\n",
    "                time_on_stage = StageLogProcessor.get_time_on_stage(stage_log)\n",
    "                if time_on_stage is not None:\n",
    "                    stage_times[stage_log.stage_name].append(time_on_stage)\n",
    "    for stage, times in stage_times.items():\n",
    "        if times:\n",
    "            median_time_per_stage_per_session[\"session\"].append(session_id)\n",
    "            median_time_per_stage_per_session[\"stage\"].append(stage.value)\n",
    "            median_time_per_stage_per_session[\"median_time\"].append(median(times))\n",
    "\n",
    "median_time_per_stage_per_session_df = DataFrame(median_time_per_stage_per_session).sort_values(by=\"session\")\n",
    "px.line(\n",
    "    median_time_per_stage_per_session_df,\n",
    "    x=\"session\",\n",
    "    y=\"median_time\",\n",
    "    color=\"stage\",\n",
    "    labels={\"session\": \"Session\", \"median_time\": \"Median time (seconds)\", \"stage\": \"PRIMMDebug stage\"},\n",
    "    title=\"Median time spent on each PRIMMDebug stage per session\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness and outcomes of exercise\n",
    "These findings relate to any metrics of success that can be measured, which are described below:\n",
    "- **Successful** challenge attempts: Where the final snapshot of a students' challenge attempt passes all the test harnesses.\n",
    "- Challenge attempts where the final snapshot of a challenge attempt successfully executes (runs without raising error messages). This is different to whether programs passed the test or not; a snapshot could run without raising errors but still be logically correct\n",
    "- **\"Entirely completed\"** challenge attempts: Where the student has reached the final *Make* stage of PRIMM. This required them to self-report their success at the *Test* stage.\n",
    "\n",
    "- Number of *Find the Error* stages where students correctly responded (this was required for all but one of the PRIMMDebug challenges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_attempts: int = len([test_report for test_report in test_reports if test_report.n_successful_tests == test_report.n_total_tests])\n",
    "print(f\"Number of attempts where final program passes test harnesses: {display_percentage_string(successful_attempts, len(test_reports))}\")\n",
    "\n",
    "final_program_states: list[bool] = [ExerciseLogProcessor.is_final_program_erroneous(exercise) for exercise in EXERCISE_LOGS]\n",
    "number_successful_final_program_states: list[bool] = len([final_program_state for final_program_state in final_program_states if final_program_state])\n",
    "print(f\"Proportion of PRIMMDebug challenges where last program run successfully executed: {display_percentage_string(number_successful_final_program_states, len(EXERCISE_LOGS))}\")\n",
    "\n",
    "number_completed_exercises: int = len([exercise_log for exercise_log in EXERCISE_LOGS if ExerciseLogProcessor.get_last_stage(exercise_log).stage_name == DebuggingStage.modify])\n",
    "print(f\"Number of entirely completed PRIMMDebug challenges (where students reached the Make stage): {display_percentage_string(number_completed_exercises, len(EXERCISE_LOGS))}\\n\")\n",
    "\n",
    "find_error_stages_with_correct_field: list[StageLog] = [stage_log for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.find_error and stage_log.correct is not None]\n",
    "correct_find_error_stages: int = len([stage_log for stage_log in find_error_stages_with_correct_field if stage_log.correct])\n",
    "print(f\"Number of find the error stages where the correct response was entered (for challenges where students had to pinpoint a line): {display_percentage_string(correct_find_error_stages, len(find_error_stages_with_correct_field))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in correctness\n",
    "These figures give more context as to how some of these measures change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change in correctness over time\n",
    "correctness_per_session = {\"session\": [], \"correctness\": [], \"total\": [], \"percent_correct\": []}\n",
    "\n",
    "for session_id, logs in EXERCISE_LOGS_PER_SESSION.items():\n",
    "    logs_with_test_report: list[ExerciseLog] = [log for log in logs if hasattr(log, \"test_report\") and log.test_report is not None]\n",
    "    correctness_per_session[\"session\"].append(session_id)\n",
    "    n_correct_exercises: int = len([log for log in logs_with_test_report if log.test_report.n_successful_tests == log.test_report.n_total_tests])\n",
    "    correctness_per_session[\"percent_correct\"].append((n_correct_exercises / len(logs_with_test_report) * 100) if logs_with_test_report else 0)\n",
    "    correctness_per_session[\"correctness\"].append(n_correct_exercises)\n",
    "    correctness_per_session[\"total\"].append(len(logs_with_test_report))\n",
    "\n",
    "# Order the dictionary by session_id\n",
    "correctness_per_session_df: DataFrame = DataFrame(correctness_per_session).sort_values(by=\"session\")\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=correctness_per_session_df[\"session\"],\n",
    "    y=correctness_per_session_df[\"total\"],\n",
    "    mode='lines',\n",
    "    name='Total',\n",
    "    fill='tozeroy'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=correctness_per_session_df[\"session\"],\n",
    "    y=correctness_per_session_df[\"correctness\"],\n",
    "    mode='lines',\n",
    "    name='Correctness',\n",
    "    fill='tozeroy'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Change in Correctness and Total Attempts Over Sessions\",\n",
    "    xaxis_title=\"Session\",\n",
    "    yaxis_title=\"Count\",\n",
    "    legend_title=\"Metric\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "px.line(\n",
    "    correctness_per_session_df,\n",
    "    x=\"session\",\n",
    "    y=\"percent_correct\",\n",
    "    labels={\"session\": \"Session\", \"percent_correct\": \"Percent Correct\"},\n",
    "    title=\"Change in correctness over sessions\"\n",
    ").show()\n",
    "\n",
    "#Change in correct first time Find the Error stage by session?\n",
    "find_the_error_correctness_per_session = {\"session\": [], \"correctness\": [], \"total\": [], \"percent_correct\": []}\n",
    "\n",
    "for session_id, logs in EXERCISE_LOGS_PER_SESSION.items():\n",
    "    find_the_error_correctness_per_session[\"session\"].append(session_id)\n",
    "    n_correct: int = len([log for log in logs if ExerciseLogProcessor.does_find_the_error_stage_have_correct_response(log)])\n",
    "    find_the_error_correctness_per_session[\"correctness\"].append(n_correct)\n",
    "    n_exercises: int = len([log for log in logs if ExerciseLogProcessor.get_first_find_the_error_stage(log) is not None])\n",
    "    find_the_error_correctness_per_session[\"total\"].append(n_exercises)\n",
    "    find_the_error_correctness_per_session[\"percent_correct\"].append((n_correct / n_exercises * 100) if n_exercises > 0 else 0)\n",
    "\n",
    "\n",
    "find_the_error_correctness_per_session_df = DataFrame(find_the_error_correctness_per_session).sort_values(by=\"session\")\n",
    "px.line(\n",
    "    find_the_error_correctness_per_session_df,\n",
    "    x=\"session\",\n",
    "    y=\"percent_correct\",\n",
    "    labels={\"session\": \"Session\", \"percent_correct\": \"Percent Correct\"},\n",
    "    title=\"Change in correctness of first Find the Error stage over sessions (correct response entered)\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some General Engagement Considerations\n",
    "Some more measures providing some context into how much students were engaging with certain restricted or unrestricted parts of the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Students' runtime behaviour in the *Inspect The Code* and *Test* stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs_inspect_the_code: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.inspect_code]\n",
    "print(\"Number of inspect the code stages:\", len(n_runs_inspect_the_code))\n",
    "n_runs_test: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.test]\n",
    "print(\"Number of test stages:\", len(n_runs_test))\n",
    "\n",
    "n_non_zero_runs_inspect_the_code: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.inspect_code and StageLogProcessor.get_number_of_runs(stage_log) > 0]\n",
    "n_non_zero_of_runs_test: list[int] = [StageLogProcessor.get_number_of_runs(stage_log) for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.test and StageLogProcessor.get_number_of_runs(stage_log) > 0]\n",
    "\n",
    "df_runs = DataFrame({\n",
    "    \"Number of Runs\": n_runs_inspect_the_code + n_runs_test,\n",
    "    \"Stage\": ([\"Inspect the Code\"] * len(n_runs_inspect_the_code)) + ([\"Test\"] * len(n_runs_test))\n",
    "})\n",
    "df_non_zero_runs = DataFrame({\n",
    "    \"Number of Runs\": n_non_zero_runs_inspect_the_code + n_non_zero_of_runs_test,\n",
    "    \"Stage\": ([\"Inspect the Code\"] * len(n_non_zero_runs_inspect_the_code)) + ([\"Test\"] * len(n_non_zero_of_runs_test))\n",
    "})\n",
    "\n",
    "px.histogram(\n",
    "    df_non_zero_runs,\n",
    "    x=\"Number of Runs\",\n",
    "    color=\"Stage\",\n",
    "    marginal=\"box\",\n",
    "    barmode=\"overlay\",\n",
    "    labels={\"Number of Runs\": \"Number of runs\", \"count\": \"Frequency\"},\n",
    "    title=\"Number of runs for Inspect the Code and Test stages\"\n",
    ").show()\n",
    "\n",
    "labels = ['0', '1', '2', '3', '4', '5+']\n",
    "df_runs['Runs Grouped'] = cut(df_runs['Number of Runs'], bins=[-0.1, 0.9, 1.9, 2.9, 3.9, 4.9, float('inf')], labels=labels, right=True)\n",
    "px.bar(\n",
    "    df_runs.groupby(['Stage', 'Runs Grouped'], observed=True).size().reset_index(name='Count'),\n",
    "    x='Runs Grouped',\n",
    "    y='Count',\n",
    "    color='Stage',\n",
    "    barmode='group',\n",
    "    labels={\"Runs Grouped\": \"Number of Runs (Grouped)\", \"Count\": \"Frequency\"},\n",
    "    title=\"Number of runs for Inspect the Code and Test stages (Grouped)\"\n",
    ").show()\n",
    "\n",
    "time_between_runs_inspect_the_code: list[float] = [time for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.inspect_code for time in StageLogProcessor.get_time_between_runs(stage_log) if StageLogProcessor.get_time_between_runs(stage_log)]\n",
    "time_between_runs_test: list[float] = [time for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.test for time in StageLogProcessor.get_time_between_runs(stage_log) if StageLogProcessor.get_time_between_runs(stage_log)]\n",
    "df_time_between_runs = DataFrame({\n",
    "    \"Time Between Runs (seconds)\": time_between_runs_inspect_the_code + time_between_runs_test,\n",
    "    \"Stage\": ([f\"Inspect the Code (n={len(time_between_runs_inspect_the_code)})\"] * len(time_between_runs_inspect_the_code)) + ([f\"Test (n={len(time_between_runs_test)})\"] * len(time_between_runs_test))\n",
    "})\n",
    "\n",
    "px.histogram(\n",
    "    df_time_between_runs,\n",
    "    x=\"Time Between Runs (seconds)\",\n",
    "    color=\"Stage\",\n",
    "    marginal=\"box\",\n",
    "    barmode=\"overlay\",\n",
    "    labels={\"Time Between Runs (seconds)\": \"Time between runs (seconds)\", \"count\": \"Frequency\"},\n",
    "    title=\"Time between runs for Inspect the Code and Test stages)\"\n",
    ").show() #TODO: Check potential errors as graphs indicate discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In places where students had the choice of which stage to go to, where did they go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to track succeeding stages after incorrect Find the Error or Test stage\n",
    "df_succeeding_stages = DataFrame(columns=[\"Stage\", \"Succeeding Stage\"])\n",
    "\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    incorrect_find_the_error_stages: list[StageLog] = [stage_log for stage_log in exercise_log.stage_logs if stage_log.stage_name == DebuggingStage.find_error and not stage_log.correct]\n",
    "    incorrect_test_stages: list[StageLog] = [stage_log for stage_log in exercise_log.stage_logs if stage_log.stage_name == DebuggingStage.test and not stage_log.correct]\n",
    "    for stage_log in incorrect_find_the_error_stages:\n",
    "        succeeding_stage: StageLog = ExerciseLogProcessor.get_succeeding_stage(exercise_log, stage_log)\n",
    "        if succeeding_stage is not None:\n",
    "            df_succeeding_stages.loc[len(df_succeeding_stages)] = [DebuggingStage.find_error.name, succeeding_stage.stage_name.name]\n",
    "    for stage_log in incorrect_test_stages:\n",
    "        succeeding_stage: StageLog = ExerciseLogProcessor.get_succeeding_stage(exercise_log, stage_log)\n",
    "        if succeeding_stage is not None:\n",
    "                        df_succeeding_stages.loc[len(df_succeeding_stages)] = [DebuggingStage.test.name, succeeding_stage.stage_name.name]\n",
    "\n",
    "\n",
    "px.bar(\n",
    "    df_succeeding_stages,\n",
    "    color=\"Stage\",\n",
    "    barmode=\"group\",\n",
    "    labels={\"count\": \"Frequency\", \"value\": \"Succeeding Stage\"},\n",
    "    title=\"Succeeding stage after incorrect Find the Error or Test stage\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the *Test* stages where students reported their code as correct, what was the actual correctness of students' changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_self_reported_correct_test_stages: int = 0\n",
    "n_actual_correct_test_stages: int = 0\n",
    "\n",
    "docker_interface: DockerInterface = DockerInterface.get_instance()\n",
    "docker_interface.create_docker_container()\n",
    "\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    correct_test_stages: list[StageLog] = [stage_log for stage_log in exercise_log.stage_logs if stage_log.stage_name == DebuggingStage.test and stage_log.correct]\n",
    "    for stage_log in correct_test_stages:\n",
    "        if stage_log.program_logs is not None and len(stage_log.program_logs) > 0:\n",
    "            n_self_reported_correct_test_stages += 1\n",
    "            last_snapshot_in_stage: str = stage_log.program_logs[-1].snapshot\n",
    "            test_report: TestReport = docker_interface.test_student_program(last_snapshot_in_stage, exercise_log.student_id, exercise_log.exercise_name)\n",
    "            if test_report.n_successful_tests == test_report.n_total_tests:\n",
    "                n_actual_correct_test_stages += 1\n",
    "\n",
    "docker_interface.close_docker_container()\n",
    "print(f\"Number of actual correct Test stages for students who self-reported correctness on the Test stage: {display_percentage_string(n_actual_correct_test_stages, n_self_reported_correct_test_stages)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students who reported being successful vs. unsuccessful in the test stage. Note that this was not properly logged due to an error in the logging system for the tool. Therefore, we have inferred students' response where possible, but this lowers the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_test_stages: list[StageLog] = [stage_log for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.test and stage_log.correct]\n",
    "incorrect_test_stages: list[StageLog] = [stage_log for stage_log in STAGE_LOGS if stage_log.stage_name == DebuggingStage.test and not stage_log.correct]\n",
    "print(f\"Number of correct Test stages: {len(correct_test_stages)}\")\n",
    "correct_test_stages_with_zero_runs: list[StageLog] = [stage_log for stage_log in correct_test_stages if StageLogProcessor.get_number_of_runs(stage_log) == 0]\n",
    "print(f\"Number of correct Test stages with zero runs: {len(correct_test_stages_with_zero_runs)}\")\n",
    "print(f\"Number of incorrect Test stages: {len(incorrect_test_stages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Between Log Data and Survey Responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis\n",
    "\n",
    "The exercise logs are structured in such a way that could benefit from clustering challenge attempts based on common pattern. Given the study's focus on reflective debugging, we decided to cluster based on the time spent on each PRIMMDebug stage. Then, the success, quality of written responses, and survey responses can be calculated to see any difference between the clusters.\n",
    "\n",
    "Procedure for clustering analysis was informed by Frades and Matthiesen (2010), Everitt et al., (2011) and Murphy et al., (2024). As a result, a few options for each stage was selected in order that could in turn be compared and evaluated.\n",
    "\n",
    "1. **What are we clustering**: Clustering for two main objects is being explored:\n",
    "  - Features: Median time per PRIMMDebug stage. Only the stages related to debugging were included, represented in the `columns` variable. Each of these features are continuous data.\n",
    "  - Item: PRIMMDebug challenge attempts\n",
    "2. **How does the data need to be transformed**: Some general preprocessing will need to be performed on the data.\n",
    "  - Missing data: Check how much missing data there is (lots of challenge attempts won't get to the final stages - what to do about these? Labelling them as 0 will naturally influence the clusters). Check how much missing data there is.\n",
    "  - For the partioning methods, data will need to be standardised.\n",
    "  - An item-feature matrix also needs to be made for clustering, represented in the `median_times_per_stage_df` DataFrame.\n",
    "3. **Similarity measure**: For the K-medoids and Agglomerative Hierarchical (AHC) methods, a mix of methods that work solely with continuous data, namely Euclidean, Manhattan (more robust to outliers), Minkowski are experimented with (see Appendix A).\n",
    "4. **Clustering algorithm**: Three clustering algorithms are being tried and compared: K-means, K-medoids, and AHC.\n",
    "5. **Evaluation of clusters**: Widely different solutions may indicate abscence of clusters based on current variables. Here's some metrics used, specifically based on Everitt (2011) and Studer (2013):\n",
    "   - For cluster quality: Average Silhouette Width and silhouette plot can be used to determine whether each cluster is \"good\", as well as bootstrapping.\n",
    "   - For the influence of a single point: Point biserial index (value of 0-1. The closer the value to 1).\n",
    "   - Values for both measures range from -1-1, with values closer to 1 indicating better clustering solutions.\n",
    "   - For the difference between the clusters obtained and theoretically best clusters, Hubert's C. Values range from 0-1, with 0 indicating good values.\n",
    "6. **Interpretation**: Inspect clusters and give them names based on their features.\n",
    "\n",
    "Additionally, guidance on acceptable sizez of clusters was consulted:\n",
    "- 10 x total number of variables (Qiu, 2023)\n",
    "- At least 20-30 observations in each cluster (Dalmaijer et al., 2022)\n",
    "- Siddiqui (2013) state there is no required minimum size, due to clustering being unsupervised and always producing some sort of result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a `DataFrame` that contains the features we want to cluster on. This is stored in the `median_times_per_stage_df` variable, which contains the *median* time spent on each debugging stage, *excluding* challenge attempts didn't complete every debugging stage (*n*=278).\n",
    "\n",
    "Note that we also attempted clustering per student (i.e., working out the median time that a particular student spent on each PRIMMDebug stage). However, the initial cluster evaluation metrics were low, nor were the resulting size of the clusters sufficient. We hence only report on the above DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns: list[DebuggingStage] = [column.value for column in [DebuggingStage.predict, DebuggingStage.run, DebuggingStage.spot_defect, DebuggingStage.inspect_code, DebuggingStage.find_error, DebuggingStage.fix_error, DebuggingStage.test]]\n",
    "median_times_per_stage_df: DataFrame = DataFrame(columns=columns)\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    times_per_stage: dict[DebuggingStage, float] = {stage.value: time for stage, time in ExerciseLogProcessor.get_time_per_stage(exercise_log).items() if stage.value in columns}\n",
    "    median_times_per_stage: dict[DebuggingStage, float] = {stage.value: time for stage, time in ExerciseLogProcessor.get_time_per_stage(exercise_log).items() if stage.value in columns}\n",
    "    if all(value > 0 for value in median_times_per_stage.values()):\n",
    "        median_times_per_stage_df.loc[exercise_log.id] = median_times_per_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then standardise these values using `StandardScaler.fit_transform` and plot scatter plot matrices and remove outliers (where values > 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "median_times_per_stage_df = DataFrame(scaler.fit_transform(median_times_per_stage_df), columns=columns, index=median_times_per_stage_df.index)\n",
    "\n",
    "threshold: float = 3.0\n",
    "\n",
    "median_times_per_stage_df: DataFrame = median_times_per_stage_df[(median_times_per_stage_df.abs() <= threshold).all(axis=1)]\n",
    "print(f\"Number of observations for median_times_per_stage_df: {len(median_times_per_stage_df)}\")\n",
    "median_times_per_stage_df.to_csv(\"data/temp/median_times_per_stage_df.csv\", index_label=\"exercise_log_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Clusters Using K-means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k4_means_final_model = KMeans(n_clusters=4, n_init=50, random_state=0)\n",
    "k4_means_final_model_labels = k4_means_final_model.fit_predict(median_times_per_stage_df)\n",
    "print(f\"Clusters for k=4 model: {get_size_of_clusters(k4_means_final_model_labels)}\")\n",
    "\n",
    "exercise_logs_to_clusters_k4_means: DataFrame = DataFrame()\n",
    "exercise_logs_to_clusters_k4_means.name = \"K Means (K=4)\"\n",
    "exercise_logs_to_clusters_k4_means[\"exercise_log_id\"] = median_times_per_stage_df.index\n",
    "exercise_logs_to_clusters_k4_means[\"cluster\"] = [i+1 for i in k4_means_final_model_labels]\n",
    "\n",
    "k4_means_final_model_centroids = DataFrame(scaler.inverse_transform(k4_means_final_model.cluster_centers_), columns=median_times_per_stage_df.columns)\n",
    "plot_median_times_per_stage_of_cluster_centroids(k4_means_final_model_centroids, model_name=\"K Means (K=4)\")\n",
    "\n",
    "#TODO: Add cluster evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the clusters\n",
    "Now we've obtained some clustering solutions, we see how exercise attempts within each cluster correlate to:\n",
    "- Success on exercise attempts (whether final program passed test harnesses)\n",
    "- Median time on PRIMMDebug challenge\n",
    "- Median time on each PRIMMDebug stage\n",
    "- Number of stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correctness_of_clusters(exercise_logs_to_clusters_k4_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k4_means.name)\n",
    "plot_median_times_per_stage_of_clusters(exercise_logs_to_clusters_k4_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k4_means.name)\n",
    "plot_median_times_per_exercise_of_clusters(exercise_logs_to_clusters_k4_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k4_means.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Everitt, B.S., Landau, S., Leese, M. and Stahl, D., 2011. Cluster analysis 5th edition Wiley.\n",
    "- Frades I, Matthiesen R. Overview on techniques in cluster analysis. Methods Mol Biol. 2010;593:81-107. doi: 10.1007/978-1-60327-194-3_5. PMID: 19957146.\n",
    "- Kassambara, A. (2017). Practical guide to cluster analysis in R: Unsupervised machine learning (Vol. 1). Sthda.\n",
    "- Keefe Murphy, Sonsoles López-Pernas, Mohammed Saqr (2024). Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R. In M. Saqr & S. López-Pernas (Eds.), Learning analytics methods and tutorials: A practical guide using R   (pp. 231-283).Springer, Cham. doi: 10.1007/978-3-031-54464-4_8\n",
    "- Saqr, M. et al. (2024). Sequence Analysis in Education: Principles, Technique, and Tutorial with R. In: Saqr, M., López-Pernas, S. (eds) Learning Analytics Methods and Tutorials. Springer, Cham. https://doi.org/10.1007/978-3-031-54464-4_10\n",
    "- Studer, Matthias (2013). WeightedCluster Library Manual: A practical guide to creating typologies of trajectories in the social sciences with R. LIVES Working Papers, 24. DOI: http://dx.doi.org/10.12682/lives.2296-1658. 2013.24.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Full Clustering Procedure\n",
    "\n",
    "The code below details the entire clustering procedure, from creating the dataset to deciding on which clustering model to use. This isn't in the main body of the report for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns: list[DebuggingStage] = [column.value for column in [DebuggingStage.predict, DebuggingStage.run, DebuggingStage.spot_defect, DebuggingStage.inspect_code, DebuggingStage.find_error, DebuggingStage.fix_error, DebuggingStage.test]]\n",
    "median_times_per_stage_df: DataFrame = DataFrame(columns=columns)\n",
    "for exercise_log in EXERCISE_LOGS:\n",
    "    times_per_stage: dict[DebuggingStage, float] = {stage.value: time for stage, time in ExerciseLogProcessor.get_time_per_stage(exercise_log).items() if stage.value in columns}\n",
    "    median_times_per_stage: dict[DebuggingStage, float] = {stage.value: time for stage, time in ExerciseLogProcessor.get_time_per_stage(exercise_log).items() if stage.value in columns}\n",
    "    if all(value > 0 for value in median_times_per_stage.values()):\n",
    "        median_times_per_stage_df.loc[exercise_log.id] = median_times_per_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then standardise these values using `StandardScaler.fit_transform` and plot scatter plot matrices to identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "median_times_per_stage_df = DataFrame(scaler.fit_transform(median_times_per_stage_df), columns=columns, index=median_times_per_stage_df.index)\n",
    "px.scatter_matrix(median_times_per_stage_df, dimensions=median_times_per_stage_df.columns, title=\"Scatter matrix of non-zero median times per stage\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing outliers**\n",
    "\n",
    "Each scatter plot matrix shows a few outliers, which are subsequently set to the threshold = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold: float = 3.0\n",
    "\n",
    "median_times_per_stage_without_outliers: DataFrame = median_times_per_stage_df[(median_times_per_stage_df.abs() <= threshold).all(axis=1)]\n",
    "print(f\"Number of observations for median_times_per_stage_without_outliers: {len(median_times_per_stage_without_outliers)}\")\n",
    "median_times_per_stage_without_outliers.to_csv(\"data/temp/median_times_per_stage_without_outliers.csv\", index_label=\"exercise_log_id\")\n",
    "\n",
    "median_times_per_stage_with_outliers = median_times_per_stage_df.clip(upper=threshold)\n",
    "print(f\"Number of observations for median_times_per_stage_with_outliers: {len(median_times_per_stage_with_outliers)}\")\n",
    "median_times_per_stage_with_outliers.to_csv(\"data/temp/median_times_per_stage_with_outliers.csv\", index_label=\"exercise_log_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should Clustering be Applied?\n",
    "Apply the following tests to the Dataframes based on guidance in Kassambara (2017):\n",
    "- Hopkins statistic\n",
    "- Visual assessment of cluster tendency (VAT)\n",
    "\n",
    "*`rpy2` is currently having issues, so as a temporary workaround this analysis is being done in an R jupyter notebook, so we save the relevant DF here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "factoextra = importr('factoextra')\n",
    "\n",
    "with (rpy2.robjects.default_converter + pandas2ri.converter).context():\n",
    "  median_times_per_stage_df_r = rpy2.robjects.conversion.get_conversion().py2rpy(median_times_per_stage_df)\n",
    "\n",
    "clustering_tendency = factoextra.get_clust_tendency(median_times_per_stage_df_r, n=median_times_per_stage_df.shape[0] - 1)\n",
    "print(f\"Hopkins statistic: {clustering_tendency.rx2('hopkins_stat')[0]}\")\n",
    "\n",
    "v_dist = clustering_tendency.rx2(\"plot\")[0]\n",
    "#image_png(v_dist)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the Hopkins stat is close to 1 (especially compared to 0.50), the data is significantly clusterable (Kassambara, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "Disadvantages:\n",
    "- `k` has to be determined in advanced (although visual heuristics can be used to help optimise this Murphy et al., (2024)).\n",
    "- Constrained to Euclidean distance. This is not a significant drawback as this was defined as a suitable distance metric given the continuous nature of the data.\n",
    "- Sensitive to initial clusters and can converge to local minima. Can be done by running the algorithm within a large number of random centroids.\n",
    "\n",
    "First, we work out the best value of k from elbow plot from visualising elbow plot of TWCSS, as well as taking into account the average silhouette values in a separate plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K: int = 10\n",
    "k_means_solutions: DataFrame = DataFrame(columns=[\"k\", \"Total within-cluster sum of squares\", \"Average silhouette width\"])\n",
    "\n",
    "i: int = 0\n",
    "for k in range(2, K + 1):\n",
    "    k_means = KMeans(n_clusters=k, random_state=0).fit(median_times_per_stage_without_outliers)\n",
    "    average_silhouette_score = metrics.silhouette_score(median_times_per_stage_without_outliers, k_means.labels_)\n",
    "    k_means_solutions.loc[i] = [k, k_means.inertia_, average_silhouette_score]\n",
    "    i += 1\n",
    "\n",
    "px.line(\n",
    "    k_means_solutions,\n",
    "    x = \"k\",\n",
    "    y = \"Total within-cluster sum of squares\",\n",
    "    title=\"Elbow plot for k-means clustering\"\n",
    ").show()\n",
    "\n",
    "px.line(\n",
    "    k_means_solutions,\n",
    "    x = \"k\",\n",
    "    y = \"Average silhouette width\",\n",
    "    title=\"Change in ASW for k-means clustering\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning\n",
    "There is not a massively clear elbow in the first plot, but some stabilisation appears to happen at K=3 and K=4, with K=4 having a much higher ASW. We therefore move forwards with both of these K-values, attempting to improve the models using different `init` and `n_init` parameter values to see if the TWCSS can be improved any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range (3, 5):\n",
    "    for init in [\"k-means++\", \"random\"]:\n",
    "        for n_init in [1, 50, 100]:\n",
    "            k_means = KMeans(n_clusters=k, init=init, n_init=n_init, random_state=0).fit(median_times_per_stage_without_outliers) #random_state=0 ensures reproducibility but inflates the value of TWCSS; is this good>\n",
    "            print(f\"Exercise attempts: k={k}: {init}, {n_init}: {k_means.inertia_}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Final Models\n",
    "Two models with the following parameters were selected:\n",
    "- `DataFrame=exercises, k=3, init=k-means++ (default argument), n_init=50`\n",
    "- `DataFrame=exercises, k=4, init=k-means++ (default argument), n_init=50`\n",
    "\n",
    "We evaluate these next to decide whether their clusters are worth investigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3_means_final_model = KMeans(n_clusters=3, n_init=50, random_state=0)\n",
    "k3_means_final_model_labels = k3_means_final_model.fit_predict(median_times_per_stage_without_outliers)\n",
    "print(f\"Clusters for k=3 model: {get_size_of_clusters(k3_means_final_model_labels)}\")\n",
    "\n",
    "k4_means_final_model = KMeans(n_clusters=4, n_init=50, random_state=0)\n",
    "k4_means_final_model_labels = k4_means_final_model.fit_predict(median_times_per_stage_without_outliers)\n",
    "print(f\"Clusters for k=4 model: {get_size_of_clusters(k4_means_final_model_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Internal Cluster Quality\n",
    "To decide whether to take these candidate solutions forward, we work out some evaluative statistics of these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame({\"exercise_log_id\": median_times_per_stage_without_outliers.index, \"label\": k3_means_final_model_labels}).to_csv(\"data/temp/k3_means_final_model_labels.csv\", index=False)\n",
    "DataFrame({\"exercise_log_id\": median_times_per_stage_without_outliers.index, \"label\": k4_means_final_model_labels}).to_csv(\"data/temp/k4_means_final_model_labels.csv\", index=False)\n",
    "\n",
    "#k3_means_final_model_metrics = calculate_cluster_evaluation_metrics(median_times_per_stage_df, k3_means_final_model_labels)\n",
    "silhouette_values = metrics.silhouette_samples(median_times_per_stage_without_outliers, k3_means_final_model_labels)\n",
    "silhouette_scores = metrics.silhouette_score(median_times_per_stage_without_outliers, k3_means_final_model_labels)\n",
    "fig, ax = plot_silhouette_plot(k3_means_final_model_labels, silhouette_values, silhouette_scores, 3)\n",
    "plt.title(f\"Silhouette plot for K means model (K = 3)\")\n",
    "plt.show()\n",
    "\n",
    "#k4_means_final_model_metrics = calculate_cluster_evaluation_metrics(median_times_per_stage_df, k4_means_final_model_labels)\n",
    "silhouette_values = metrics.silhouette_samples(median_times_per_stage_without_outliers, k4_means_final_model_labels)\n",
    "silhouette_scores = metrics.silhouette_score(median_times_per_stage_without_outliers, k4_means_final_model_labels)\n",
    "fig, ax = plot_silhouette_plot(k4_means_final_model_labels, silhouette_values, silhouette_scores, 4)\n",
    "plt.title(f\"Silhouette plot for K means model (K = 4)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics generally favour K=4, with the only metric lower for K=3 being the ASW. The silhouette plot for K=3 clearly shows some large differences in cluster 1. The K=4 model therefore seems more useful, especially with its TWCSS, point biserial value, and Hubert's C having more favourable values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Medoids\n",
    "\n",
    "We've already standardised and removed missing values, so the first thing to do is to visualise an elbow plot. We initially also tried this with the student DataFrame but the AWS's were incredibly low (generally < 0.2).\n",
    "\n",
    "Here, we visualise different dissimilarity measures (euclidean, manhattan, and minkowski). We have to pre-create the dissimilarity matrices to do this. **TODO: Sort out random variation in this to ensure I'm getting the best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K: int = 10\n",
    "max_iter: int = 100\n",
    "k_medoids_solutions: DataFrame = DataFrame(columns=[\"k\", \"Total within-cluster sum of squares\", \"Average silhouette width\", \"Dissimilarity measure\"])\n",
    "\n",
    "i: int = 0\n",
    "for dissimilarity_measure in [\"euclidean\", \"manhattan\", \"minkowski\"]:\n",
    "    # Exercise attempts dataframe\n",
    "    df = metrics.pairwise_distances(median_times_per_stage_without_outliers, metric=dissimilarity_measure)\n",
    "    for k in range(2, K + 1):\n",
    "        k_medoids_intermediate = fastpam1(df, k, max_iter=max_iter)\n",
    "        average_silhouette_score = metrics.silhouette_score(median_times_per_stage_without_outliers, k_medoids_intermediate.labels, metric=dissimilarity_measure)\n",
    "        k_medoids_solutions.loc[i] = [k, k_medoids_intermediate.loss, average_silhouette_score, dissimilarity_measure]\n",
    "        i += 1\n",
    "\n",
    "px.line(\n",
    "    k_medoids_solutions,\n",
    "    x=\"k\",\n",
    "    y=\"Total within-cluster sum of squares\",\n",
    "    color=\"Dissimilarity measure\",\n",
    "    title=\"Elbow plot for k-medoids clustering\"\n",
    ").show()\n",
    "\n",
    "px.line(\n",
    "    k_medoids_solutions,\n",
    "    x=\"k\",\n",
    "    y=\"Average silhouette width\",\n",
    "    color=\"Dissimilarity measure\",\n",
    "    title=\"Change in ASW for k-medoids clustering\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning\n",
    "\n",
    "There's clearly not much of an elbow for this measure (and very little different between the euclidean and minkowski measures). However, the TWCSS for both of these dissimilarity measures are much lower than the K-means solutions, so the k=4 solution with euclidean distance is further explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metrics.pairwise_distances(median_times_per_stage_without_outliers, metric=\"minkowski\")\n",
    "\n",
    "for init in [\"random\", \"build\"]:\n",
    "    for max_iter in [1, 50, 100]:\n",
    "        k_medoids = fastpam1(df, medoids=4, max_iter=max_iter, init=init, random_state=0)\n",
    "        print(f\"k=4: {init}, {max_iter}: {k_medoids.loss}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_medoids_final_model = fastpam1(df, medoids=4, max_iter=50, random_state=0)\n",
    "k_medoids_final_model_labels = k_medoids_final_model.labels\n",
    "print(f\"Clusters for k=4 model: {get_size_of_clusters(k_medoids_final_model_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Internal Cluster Quality\n",
    "To decide whether to take these candidate solutions forward, we work out some evaluative statistics of these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_values = metrics.silhouette_samples(median_times_per_stage_without_outliers, k_medoids_final_model_labels)\n",
    "silhouette_scores = metrics.silhouette_score(median_times_per_stage_without_outliers, k_medoids_final_model_labels)\n",
    "fig, ax = plot_silhouette_plot(k_medoids_final_model_labels, silhouette_values, silhouette_scores, 4)\n",
    "plt.title(f\"Silhouette plot for K medoids model (K = 4)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Hierarchical Clustering\n",
    "\n",
    "Procedure:\n",
    "- Try a variation of linkage criteria and dissimilarity measures.\n",
    "- Visualise the dendrogram and cut at the most appropriate places (could cut at several points).\n",
    "\n",
    "After a manual inspection the following combinations of parameters:\n",
    "- Any model with single linkage\n",
    "- Average linkage and city block dissimilarity\n",
    "- Average linkage and minkowski dissimilarity\n",
    "\n",
    "**Do standardised values need to be used here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_methods: list[str] = [\"single\",\"complete\", \"average\", \"ward\"]\n",
    "for dissimilarity_measure in [\"euclidean\", \"cityblock\", \"minkowski\"]:\n",
    "    for linkage_method in linkage_methods:\n",
    "        if (linkage_method == \"ward\" and dissimilarity_measure != \"euclidean\"):\n",
    "            continue  # Ward only works with Euclidean distance\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        linkage_model = linkage(median_times_per_stage_without_outliers, method=linkage_method, metric=dissimilarity_measure)\n",
    "        plt.title(f\"Dendrogram for {linkage_method} linkage with {dissimilarity_measure} dissimilarity\")\n",
    "        dendrogram(linkage_model)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting candidate solutions\n",
    "\n",
    "Now we have plotted several dendrograms, we can remove some through visual inspection:\n",
    "- All solutions where `linkage_method = \"single\"` produces undesirable chaining effects.\n",
    "- `{linkage_method: \"average\", dissimilarity_measure: \"euclidean\"}` as this generates a huge cluster (coloured red in the dendogram) and several other very small clusters.\n",
    "- `{linkage_method: \"average\", dissimilarity_measure: \"cityblock\"}` produces an undesirable chaining effect.\n",
    "- `{linkage_method: \"average\", dissimilarity_measure: \"minkowski\"}` suffers from the same problem as the cluster with `{linkage_method: \"average\", dissimilarity_measure: \"euclidean\"}`.\n",
    "\n",
    "This means we move on with the models in the `candidate_models` list below. Some of these could be cut at different points; the number of clusters that should be evaluated is indicated by the `k` variable in `candidate_models`. We generate cluster models based on each of these and evaluate them using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_models: list[dict[str, str]] = [{\n",
    "    \"linkage_method\": \"complete\",\n",
    "    \"dissimilarity_measure\": \"euclidean\",\n",
    "    \"n_clusters\": [3]\n",
    "},\n",
    "{\n",
    "    \"linkage_method\": \"ward\",\n",
    "    \"dissimilarity_measure\": \"euclidean\",\n",
    "    \"n_clusters\": [2, 3, 4]\n",
    "},\n",
    "{\n",
    "    \"linkage_method\": \"complete\",\n",
    "    \"dissimilarity_measure\": \"cityblock\",\n",
    "    \"n_clusters\": [3, 4]\n",
    "},\n",
    "{\n",
    "    \"linkage_method\": \"complete\",\n",
    "    \"dissimilarity_measure\": \"minkowski\",\n",
    "    \"n_clusters\": [2, 3]\n",
    "}]\n",
    "\n",
    "for model in candidate_models:\n",
    "    for n_clusters in model[\"n_clusters\"]:\n",
    "        linkage_model = linkage(median_times_per_stage_without_outliers, method=model[\"linkage_method\"], metric=model[\"dissimilarity_measure\"])\n",
    "        cluster_labels = fcluster(linkage_model, n_clusters, criterion='maxclust')\n",
    "        silhouette_values = metrics.silhouette_samples(median_times_per_stage_without_outliers, cluster_labels)\n",
    "        silhouette_scores = metrics.silhouette_score(median_times_per_stage_without_outliers, cluster_labels)\n",
    "        fig, ax = plot_silhouette_plot(cluster_labels, silhouette_values, silhouette_scores, 4)\n",
    "        plt.title(f\"Silhouette plot for {model['linkage_method']} linkage with {model['dissimilarity_measure']} dissimilarity (K = {n_clusters})\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Internal Cluster Quality\n",
    "Now we've experimented with different clusters, we narrow down our clustering models and perform some final evaluation on the following AHC\n",
    "- Ward linkage (K = 4)\n",
    "- Complete linkage with cityblock similarity (K = 4)\n",
    "\n",
    "We calculate the following evaluation metrics for each clustering model.\n",
    "- Average Silhouette Width\n",
    "- Point Biserial Correlation\n",
    "- Hubert's C\n",
    "In order to calculate the latter two metrics, we need to import the weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weightedcluster = importr('WeightedCluster')\n",
    "\n",
    "ahc_ward_model = linkage(median_times_per_stage_without_outliers, method=\"ward\")\n",
    "ahc_ward_model_labels = fcluster(ahc_ward_model, 4, criterion='maxclust')\n",
    "#ahc_ward_model_labels_metrics = calculate_cluster_evaluation_metrics(median_time_per_stage_without_outliers, ahc_ward_model_labels)\n",
    "\n",
    "ahc_complete_linkage_cityblock = linkage(median_times_per_stage_without_outliers, method=\"ward\")\n",
    "ahc_complete_linkage_cityblock_labels = fcluster(ahc_complete_linkage_cityblock, 4, criterion='maxclust')\n",
    "#ahc_complete_linkage_cityblock_metrics = calculate_cluster_evaluation_metrics(median_time_per_stage_without_outliers, ahc_complete_linkage_cityblock_labels, \"cityblock\")\n",
    "\n",
    "\"\"\"\n",
    "for metrics in [k_means_final_model_metrics, ahc_ward_model_labels_metrics, ahc_complete_linkage_cityblock_metrics]:\n",
    "    print(\"Evaluation metrics:\")\n",
    "    print(f\"Point Biserial correlation coefficient: {metrics[\"point_biserial_correlation\"]:.2f}\")\n",
    "    print(f\"Average silhouette width: {metrics[\"average_silhouette_width\"]:.2f}\")\n",
    "    print(f\"Hubert's C: {metrics[\"huberts_c\"]:.2f}\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({\"exercise_log_id\": median_times_per_stage_without_outliers.index, \"label\": ahc_ward_model_labels}).to_csv(\"data/temp/ahc_ward_model_labels.csv\", index=False)\n",
    "pd.DataFrame({\"exercise_log_id\": median_times_per_stage_without_outliers.index, \"label\": ahc_complete_linkage_cityblock_labels}).to_csv(\"data/temp/ahc_complete_linkage_cityblock_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics of these two clusters are again similar, so before deciding whether any of these should be used, we perform further evaluation through comparing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Based Clustering\n",
    "Currently being implemented in `r_clustering.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation: Comparing Clusters\n",
    "We now have a set of cluster models listed below:\n",
    "- K Means model (K = 3)\n",
    "- K Means model (K = 4)\n",
    "- Ward linkage (K = 4)\n",
    "- Complete linkage with cityblock similarity (K = 4)\n",
    "\n",
    "We compare the cluster centroids (for the K-means models) and the median time spent on each PRIMMDebug stage (for the AHC models) to see if they are similar. This acts as a step of evaluation, as their similarity helps point to whether the clustering has consistently revealed some underlying structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_logs_to_clusters_k3_means: DataFrame = DataFrame()\n",
    "exercise_logs_to_clusters_k3_means.name = \"K Means (K=3)\"\n",
    "exercise_logs_to_clusters_k3_means[\"exercise_log_id\"] = median_times_per_stage_without_outliers.index\n",
    "exercise_logs_to_clusters_k3_means[\"cluster\"] = [i+1 for i in k3_means_final_model_labels]\n",
    "\n",
    "k3_means_final_model_centroids = DataFrame(scaler.inverse_transform(k3_means_final_model.cluster_centers_), columns=median_times_per_stage_without_outliers.columns)\n",
    "plot_median_times_per_stage_of_cluster_centroids(k3_means_final_model_centroids, model_name=\"K Means (K=3)\")\n",
    "\n",
    "exercise_logs_to_clusters_k4_means: DataFrame = DataFrame()\n",
    "exercise_logs_to_clusters_k4_means.name = \"K Means (K=4)\"\n",
    "exercise_logs_to_clusters_k4_means[\"exercise_log_id\"] = median_times_per_stage_without_outliers.index\n",
    "exercise_logs_to_clusters_k4_means[\"cluster\"] = [i+1 for i in k4_means_final_model_labels]\n",
    "\n",
    "k4_means_final_model_centroids = DataFrame(scaler.inverse_transform(k4_means_final_model.cluster_centers_), columns=median_times_per_stage_without_outliers.columns)\n",
    "plot_median_times_per_stage_of_cluster_centroids(k4_means_final_model_centroids, model_name=\"K Means (K=4)\")\n",
    "\n",
    "exercise_logs_to_clusters_ahc_ward: DataFrame = DataFrame()\n",
    "exercise_logs_to_clusters_ahc_ward[\"exercise_log_id\"] = median_times_per_stage_without_outliers.index\n",
    "exercise_logs_to_clusters_ahc_ward[\"cluster\"] = ahc_ward_model_labels\n",
    "plot_median_times_per_stage_of_clusters(exercise_logs_to_clusters_ahc_ward, EXERCISE_LOGS, \"AHC with Ward\")\n",
    "\n",
    "\n",
    "exercise_logs_to_clusters_ahc_complete_linkage: DataFrame = DataFrame()\n",
    "exercise_logs_to_clusters_ahc_complete_linkage[\"exercise_log_id\"] = median_times_per_stage_without_outliers.index\n",
    "exercise_logs_to_clusters_ahc_complete_linkage[\"cluster\"] = ahc_complete_linkage_cityblock_labels\n",
    "plot_median_times_per_stage_of_clusters(exercise_logs_to_clusters_ahc_complete_linkage, EXERCISE_LOGS, model_name=\"AHC Complete Linkage (Cityblock)\")\n",
    "\n",
    "exercise_logs_to_clusters_model_based: DataFrame = read_csv(\"data/temp/model_based_cluster_labels.csv\")\n",
    "exercise_logs_to_clusters_model_based.name = \"Model-Based Clustering\"\n",
    "\n",
    "median_times_per_stage_model_based: DataFrame = read_csv(\"data/temp/model_based_cluster_means.csv\", index_col=0)\n",
    "median_times_per_stage_model_based = DataFrame(scaler.inverse_transform(median_times_per_stage_model_based), index=median_times_per_stage_model_based.index, columns=median_times_per_stage_model_based.columns)\n",
    "plot_median_times_per_stage_of_cluster_centroids(median_times_per_stage_model_based, model_name=\"Model-Based Clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A positive sign is that for each of the 4-cluster models, the average times for each stage is similar. This acts as further confirmation of the applicability of a clustering solution where K=4. We now move onto to selecting a final model to properly investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating different cluster solutions\n",
    "Now we've obtained some clustering solutions, we see how exercise attempts within each cluster correlate to:\n",
    "- Success on exercise attempts (whether final program passed test harnesses)\n",
    "- Median time on PRIMMDebug challenge\n",
    "- Median time on each PRIMMDebug stage\n",
    "- Number of stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Final Cluster Solution\n",
    "TODO: Justify why K-4 means was chosen (model-based didn't demonstrate to other clusters, and this had most positive cluster metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of clusters: {get_size_of_clusters(k3_means_final_model_labels)}\")\n",
    "\n",
    "plot_correctness_of_clusters(exercise_logs_to_clusters_k3_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k3_means.name)\n",
    "plot_median_times_per_stage_of_clusters(exercise_logs_to_clusters_k3_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k3_means.name)\n",
    "plot_median_times_per_exercise_of_clusters(exercise_logs_to_clusters_k3_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k3_means.name)\n",
    "\n",
    "plot_correctness_of_clusters(exercise_logs_to_clusters_k4_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k4_means.name)\n",
    "plot_median_times_per_stage_of_clusters(exercise_logs_to_clusters_k4_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k4_means.name)\n",
    "plot_median_times_per_exercise_of_clusters(exercise_logs_to_clusters_k4_means, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_k4_means.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_logs_to_clusters_model_based: DataFrame = read_csv(\"data/temp/model_based_cluster_labels.csv\")\n",
    "exercise_logs_to_clusters_model_based.name = \"Model-Based Clustering\"\n",
    "\n",
    "print(f\"Size of clusters: {get_size_of_clusters(exercise_logs_to_clusters_model_based['cluster'])}\")\n",
    "\n",
    "plot_correctness_of_clusters(exercise_logs_to_clusters_model_based, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_model_based.name)\n",
    "plot_median_times_per_stage_of_clusters(exercise_logs_to_clusters_model_based, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_model_based.name)\n",
    "plot_median_times_per_exercise_of_clusters(exercise_logs_to_clusters_model_based, EXERCISE_LOGS, model_name=exercise_logs_to_clusters_model_based.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "Based on the clusters being inconclusive, we wanted to confirm the relationship between the time spent on different PRIMMDebug stages (and the challenges as a whole) with success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_per_stage_success = DataFrame(scaler.inverse_transform(median_times_per_stage_with_outliers), index=median_times_per_stage_with_outliers.index, columns=median_times_per_stage_with_outliers.columns).copy()\n",
    "exercise_log_id_to_log = {log.id: log for log in EXERCISE_LOGS if hasattr(log, \"test_report\") and log.test_report is not None}\n",
    "time_per_stage_success[\"success\"] = [\n",
    "    (exercise_log_id_to_log[log_id].test_report.n_successful_tests == exercise_log_id_to_log[log_id].test_report.n_total_tests)\n",
    "    if log_id in exercise_log_id_to_log else np.nan\n",
    "    for log_id in time_per_stage_success.index\n",
    "]\n",
    "stage_success = time_per_stage_success[\"success\"]\n",
    "\n",
    "for stage in columns:\n",
    "    stage_values = time_per_stage_success[stage]\n",
    "    pointbiserialr_coefficient, p_value = pointbiserialr(stage_values, stage_success)\n",
    "    print(f\"Point Biserial correlation coefficient for {stage}: {pointbiserialr_coefficient:.2f}, p-value: {p_value:.4f}\")\n",
    "print()\n",
    "\n",
    "#Correlation between time on challenge and correctness\n",
    "time_on_challenge_success = DataFrame(columns=[\"time_on_challenge\", \"success\"])\n",
    "for log in EXERCISE_LOGS:\n",
    "    if hasattr(log, \"test_report\") and log.test_report is not None:\n",
    "        time_on_challenge_success.loc[log.id] = [\n",
    "            ExerciseLogProcessor.get_time_on_exercise(log),\n",
    "            (log.test_report.n_successful_tests == log.test_report.n_total_tests)\n",
    "        ]\n",
    "pointbiserialr_coefficient, p_value = pointbiserialr(time_on_challenge_success[\"time_on_challenge\"], time_on_challenge_success[\"success\"])\n",
    "print(f\"Point Biserial correlation coefficient for time on challenge and correctness: {pointbiserialr_coefficient:.2f}, p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Save Logs Documentation\n",
    "Some extra documentation on how the logs are fetched from the Firestore database, parsed, and saved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
